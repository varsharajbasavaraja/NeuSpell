{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varsharajbasavaraja/Neuspell/blob/main/ECE570NeuSpell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkPEKIxn_qgl"
      },
      "source": [
        "# **Enhancing Language Precision: Context-Aware Spelling Correction**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmhCp8w479lE"
      },
      "source": [
        "## Steps to implement NeuSpell: A Neural Spelling Correction Toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1ZMQWgep-xs"
      },
      "source": [
        "###  Contents\n",
        "---\n",
        "1. Installation\n",
        "   \n",
        "    1.1 Cloning the NeuSpell github repository by mounting google drive\n",
        "    \n",
        "    1.2 Installing extra requirements and Resolving Installation Errors by changing torch version\n",
        "2. Download Checkpoints\n",
        "3. Download Datasets\n",
        "4. Implementation\n",
        "  \n",
        "    4.1 Defining the listed Models in NeuSpell\n",
        "\n",
        "    4.2 Installing Correctors\n",
        "\n",
        "      4.2.1 SC-LSTM Corrector\n",
        "\n",
        "      4.2.2 Nested-LSTM Corrector\n",
        "    \n",
        "      4.2.3 CNN-LSTM Corrector\n",
        "    \n",
        "      4.2.4 BERT Corrector\n",
        "\n",
        "5. Commmand line Interface\n",
        "  \n",
        "    5.1 SC-LSTM Checker\n",
        "\n",
        "    5.2 Nested-LSTM Checker\n",
        "\n",
        "    5.3 CNN-LSTM Checker\n",
        "\n",
        "    5.4 BERT Checker\n",
        "\n",
        " 6. Testing the Neuspell Corrector Modules SclstmChecker, NestedlstmChecker, CnnlstmChecker, BertChecker (JFLEG) dataset\n",
        "\n",
        " 7. Performance of Neuspell models (Discussion and Conclusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNBznqHzlxf1"
      },
      "source": [
        "\n",
        "# 1. Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM5D9sPy_zC6"
      },
      "source": [
        "## 1.1 Cloning the NeuSpell github repository by mounting google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IktzI_Vb_Mgl",
        "outputId": "c13e1c39-4e33-4812-c09b-40f747e09216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "Changed CWD to \"/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha\"\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root = '/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha'\n",
        "import os\n",
        "if not os.path.isdir(root):\n",
        "  os.mkdir(root)\n",
        "os.chdir(root)\n",
        "print(f'\\nChanged CWD to \"{root}\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhlfDiMjWWyj",
        "outputId": "439483fb-077a-4cfa-e472-a4aec53f8658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'neuspell' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell\n",
            "Obtaining file:///content/drive/MyDrive/Colab%20Notebooks/ECE570/Varsha/neuspell\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from neuspell==1.0.0) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from neuspell==1.0.0) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from neuspell==1.0.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from neuspell==1.0.0) (1.23.5)\n",
            "Collecting jsonlines (from neuspell==1.0.0)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting sentencepiece (from neuspell==1.0.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch_pretrained_bert (from neuspell==1.0.0)\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->neuspell==1.0.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->neuspell==1.0.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->neuspell==1.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->neuspell==1.0.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->neuspell==1.0.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->neuspell==1.0.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->neuspell==1.0.0) (2.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->neuspell==1.0.0) (23.1.0)\n",
            "Collecting boto3 (from pytorch_pretrained_bert->neuspell==1.0.0)\n",
            "  Downloading boto3-1.29.4-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert->neuspell==1.0.0) (2.31.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert->neuspell==1.0.0) (2023.6.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers->neuspell==1.0.0) (0.19.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->neuspell==1.0.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->neuspell==1.0.0) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->neuspell==1.0.0) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->neuspell==1.0.0) (0.4.0)\n",
            "Collecting botocore<1.33.0,>=1.32.4 (from boto3->pytorch_pretrained_bert->neuspell==1.0.0)\n",
            "  Downloading botocore-1.32.4-py3-none-any.whl (11.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_pretrained_bert->neuspell==1.0.0)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3->pytorch_pretrained_bert->neuspell==1.0.0)\n",
            "  Downloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->neuspell==1.0.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert->neuspell==1.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert->neuspell==1.0.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert->neuspell==1.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert->neuspell==1.0.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->neuspell==1.0.0) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.33.0,>=1.32.4->boto3->pytorch_pretrained_bert->neuspell==1.0.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.33.0,>=1.32.4->boto3->pytorch_pretrained_bert->neuspell==1.0.0) (1.16.0)\n",
            "Installing collected packages: sentencepiece, jsonlines, jmespath, botocore, s3transfer, boto3, pytorch_pretrained_bert, neuspell\n",
            "  Running setup.py develop for neuspell\n",
            "Successfully installed boto3-1.29.4 botocore-1.32.4 jmespath-1.0.1 jsonlines-4.0.0 neuspell-1.0.0 pytorch_pretrained_bert-0.6.2 s3transfer-0.7.0 sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "# Clone repository and pull latest changes.\n",
        "!git clone https://github.com/neuspell/neuspell\n",
        "\n",
        "# Change to the cloned directory\n",
        "%cd neuspell\n",
        "\n",
        "# Install the package using pip\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPog89Ma_6nQ"
      },
      "source": [
        "## 1.2 Installing extra requirements and Resolving Installation Errors by changing torch version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzihCX5Wqjzy"
      },
      "source": [
        "This resolution is done by identifying conflicts at the time of implementation by upfating pip version and installing the compatible torch versions - Student's modification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtjEy3mlIWbs",
        "outputId": "f0f68388-5413-47fd-dcc5-d519df34283c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.3.1\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h53z3dpVBdr"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.pip\n",
        "!echo \"[global]\" > ~/.pip/pip.conf\n",
        "!echo \"root_user_action = ignore\" >> ~/.pip/pip.conf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSSlqrLkK_X1"
      },
      "source": [
        "Spacy models can be downloaded as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xIODy6e5889",
        "outputId": "b3174162-9647-4990-c6f6-999a0b0499f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-11-20 23:26:21.345734: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-20 23:26:21.345800: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-20 23:26:21.345849: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-20 23:26:21.358207: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-20 23:26:23.572394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-20 23:26:26.312851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-20 23:26:26.319736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-20 23:26:26.319983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaVNkZxslMh1"
      },
      "source": [
        "Install the torch version to be compatible with all the depencies (Restart runtime if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOCo0xAtWPLm",
        "outputId": "b0456a93-b926-4153-c336-81335128b7b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.12.0\n",
            "  Downloading torchvision-0.12.0-cp310-cp310-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.11.0\n",
            "  Downloading torchaudio-0.11.0-cp310-cp310-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchdata==0.3.0\n",
            "  Downloading torchdata-0.3.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.1.1\n",
            "  Downloading torchtext-0.1.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (9.4.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.3.0) (2.0.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.1.1) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (2023.7.22)\n",
            "Installing collected packages: torch, torchvision, torchtext, torchdata, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu118\n",
            "    Uninstalling torchvision-0.16.0+cu118:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu118\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.16.0\n",
            "    Uninstalling torchtext-0.16.0:\n",
            "      Successfully uninstalled torchtext-0.16.0\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.7.0\n",
            "    Uninstalling torchdata-0.7.0:\n",
            "      Successfully uninstalled torchdata-0.7.0\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.1.0+cu118\n",
            "    Uninstalling torchaudio-2.1.0+cu118:\n",
            "      Successfully uninstalled torchaudio-2.1.0+cu118\n",
            "Successfully installed torch-1.11.0 torchaudio-0.11.0 torchdata-0.3.0 torchtext-0.1.1 torchvision-0.12.0\n",
            "Collecting allennlp\n",
            "  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<1.13.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.11.0)\n",
            "Requirement already satisfied: torchvision<0.14.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.12.0)\n",
            "Collecting cached-path<1.2.0,>=1.1.3 (from allennlp)\n",
            "  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
            "Collecting fairscale==0.4.6 (from allennlp)\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.8.1)\n",
            "Collecting spacy<3.4,>=2.1.0 (from allennlp)\n",
            "  Downloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.23.5)\n",
            "Collecting tensorboardX>=1.2 (from allennlp)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from allennlp) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.10/dist-packages (from allennlp) (4.66.1)\n",
            "Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.9.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.11.3)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.10/dist-packages (from allennlp) (7.4.3)\n",
            "Collecting transformers<4.21,>=4.1 (from allennlp)\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.1.99)\n",
            "Collecting filelock<3.8,>=3.3 (from allennlp)\n",
            "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
            "Collecting lmdb>=1.2.1 (from allennlp)\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (10.1.0)\n",
            "Collecting termcolor==1.1.0 (from allennlp)\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb<0.13.0,>=0.10.0 (from allennlp)\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.19.3)\n",
            "Collecting dill>=0.3.4 (from allennlp)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting base58>=2.1.1 (from allennlp)\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting sacremoses (from allennlp)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.9.0)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.20.3)\n",
            "Requirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (5.7.1)\n",
            "Collecting jsonnet>=0.10.0 (from allennlp)\n",
            "  Downloading jsonnet-0.20.0.tar.gz (594 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rich<13.0,>=12.1 (from cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: boto3<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (1.29.4)\n",
            "Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (2.8.0)\n",
            "Collecting huggingface-hub>=0.0.16 (from allennlp)\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (23.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (2023.6.3)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (2023.7.22)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->allennlp) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.9)\n",
            "Collecting thinc<8.1.0,>=8.0.14 (from spacy<3.4,>=2.1.0->allennlp)\n",
            "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.11)\n",
            "Collecting wasabi<1.1.0,>=0.9.1 (from spacy<3.4,>=2.1.0->allennlp)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.10)\n",
            "Collecting typer>=0.4.1 (from allennlp)\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (6.4.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.4,>=2.1.0->allennlp)\n",
            "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (9.4.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers<4.21,>=4.1->allennlp)\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Collecting shortuuid>=0.5.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading sentry_sdk-1.35.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.16.0)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: botocore<1.33.0,>=1.32.4 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.32.4)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.7.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.17.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.6.0)\n",
            "Collecting commonmark<0.10.0,>=0.9.0 (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.33.0,>=1.32.4->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.61.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.5.0)\n",
            "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-1.35.0-py2.py3-none-any.whl (248 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.6/248.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: fairscale, termcolor, jsonnet, pathtools\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307221 sha256=1c27f71417c6e56bbd174e4059e868a82374ebc07d2d26c702d28aa076760826\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/58/3d/e114952ab4a8f31eb9dae230658450afff986b211a5b1f2256\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=9dd37e3d90037607c6ce9d0dd95f2ac96c234d5ff2b8f827fa6269890f83245a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.20.0-cp310-cp310-linux_x86_64.whl size=6406864 sha256=d03679f0a5edf3280f609dc5cad554558ef72290042521d61f9d52a08bd9d937\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/0d/6b/5467dd1db9332ba4bd5cf4153e2870c5f89bb4db473d989cc2\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8790 sha256=e149c9f7afcbf16bd56988c10359e9ec7cc952b6667d69c3cc85b19104d0b4f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built fairscale termcolor jsonnet pathtools\n",
            "Installing collected packages: wasabi, tokenizers, termcolor, pathtools, lmdb, jsonnet, commonmark, typer, tensorboardX, smmap, shortuuid, setproctitle, sentry-sdk, sacremoses, rich, pydantic, filelock, docker-pycreds, dill, base58, thinc, huggingface-hub, gitdb, fairscale, transformers, spacy, GitPython, wandb, cached-path, allennlp\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.2\n",
            "    Uninstalling wasabi-1.1.2:\n",
            "      Successfully uninstalled wasabi-1.1.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.3.0\n",
            "    Uninstalling termcolor-2.3.0:\n",
            "      Successfully uninstalled termcolor-2.3.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.0\n",
            "    Uninstalling typer-0.9.0:\n",
            "      Successfully uninstalled typer-0.9.0\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.7.0\n",
            "    Uninstalling rich-13.7.0:\n",
            "      Successfully uninstalled rich-13.7.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.13.1\n",
            "    Uninstalling filelock-3.13.1:\n",
            "      Successfully uninstalled filelock-3.13.1\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.12\n",
            "    Uninstalling thinc-8.1.12:\n",
            "      Successfully uninstalled thinc-8.1.12\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.19.3\n",
            "    Uninstalling huggingface-hub-0.19.3:\n",
            "      Successfully uninstalled huggingface-hub-0.19.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.3.3 which is incompatible.\n",
            "inflect 7.0.0 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.40 allennlp-2.10.1 base58-2.1.1 cached-path-1.1.6 commonmark-0.9.1 dill-0.3.7 docker-pycreds-0.4.0 fairscale-0.4.6 filelock-3.7.1 gitdb-4.0.11 huggingface-hub-0.10.1 jsonnet-0.20.0 lmdb-1.4.1 pathtools-0.1.2 pydantic-1.8.2 rich-12.6.0 sacremoses-0.1.1 sentry-sdk-1.35.0 setproctitle-1.3.3 shortuuid-1.0.11 smmap-5.0.1 spacy-3.3.3 tensorboardX-2.6.2.2 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.12.1 transformers-4.20.1 typer-0.4.2 wandb-0.12.21 wasabi-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 torchdata==0.3.0 torchtext==0.1.1\n",
        "\n",
        "!pip install allennlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clg-l6Wpv4eU"
      },
      "source": [
        "Note: The dependency error above can be ignored as they are not used in the implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rDFSa8tKNR6",
        "outputId": "9f68a592-60d7-4d20-e4a8-2a69638f58b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting elmo\n",
            "  Downloading elmo-0.1.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests[security] in /usr/local/lib/python3.10/dist-packages (from elmo) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[security]->elmo) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[security]->elmo) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[security]->elmo) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[security]->elmo) (2023.7.22)\n",
            "Building wheels for collected packages: elmo\n",
            "  Building wheel for elmo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elmo: filename=elmo-0.1.2-py3-none-any.whl size=10027 sha256=5a8e181606636877d79270b5214a962b29afac7067d469081cf8d2e40b254cdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/c1/92/c375407270dd35708752f8e5084b74efe2ca067375b31383a5\n",
            "Successfully built elmo\n",
            "Installing collected packages: elmo\n",
            "Successfully installed elmo-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install elmo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RBocfznPJiU",
        "outputId": "25a35e29-73e6-41f3-93db-018b69605126"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files removed: 203\n"
          ]
        }
      ],
      "source": [
        "!pip cache purge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iy32-PQBSIy"
      },
      "source": [
        "# 2. Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLc_IDjetncB",
        "outputId": "6e208d74-78c6-4825-bc62-2a3a7d68dab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/data/traintest\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/data/traintest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka_S5wWP_VoI",
        "outputId": "e1eb6ba5-b7d5-44c3-c5f8-dce2819b4b61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./wo_context already exists\n"
          ]
        }
      ],
      "source": [
        "#taken from https://github.com/nsadawi/Download-Large-File-From-Google-Drive-Using-Python\n",
        "#taken from this StackOverflow answer: https://stackoverflow.com/a/39225039\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)\n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def create_paths(path_: str):\n",
        "    if not os.path.exists(path_):\n",
        "        os.makedirs(path_)\n",
        "        print(f\"{path_} created\")\n",
        "    else:\n",
        "        print(f\"{path_} already exists\")\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "\n",
        "    \"\"\"\n",
        "    All files from https://drive.google.com/drive/folders/1ejKSkiHNOlupxXVDMg67rPdqwowsTq1i?usp=sharing\n",
        "    will be downloaded to the current folder directory\n",
        "    \"\"\"\n",
        "\n",
        "    # ./\n",
        "\n",
        "    download_file_from_google_drive('1ZlEQKf3HMMk66F7DGFPnh-PA2cbt5K0F', 'test.1blm')\n",
        "    download_file_from_google_drive('1wZ6nrIYANNN3ZoHgacIg9P3UmHnBb9Wa', 'test.1blm.noise.prob')\n",
        "    download_file_from_google_drive('1epwQQjmOZyZL1ptc9mcIFjnwS0vs7L46', 'test.1blm.noise.random')\n",
        "    download_file_from_google_drive('1aT3mUfsNtTl51vc-V7kJZeflxZ4BMicD', 'test.1blm.noise.word')\n",
        "\n",
        "    download_file_from_google_drive('1QxVnFgp0pgEWmS-113SWEjT8tEhXCVF5', 'test.bea4k')\n",
        "    download_file_from_google_drive('1pnCU3OUSE0lNN1T6qY4WWhtHZsW3cg1c', 'test.bea4k.noise')\n",
        "\n",
        "    download_file_from_google_drive('1eXrAPKzfU7E9EZNKMyyanuxL9NMpkvdv', 'test.bea20k')\n",
        "    download_file_from_google_drive('178AWu05IzYFBOFYQ0lhkkBQaIACSJzAC', 'test.bea20k.noise')\n",
        "\n",
        "    download_file_from_google_drive('10VtrEThrDIiuFJf0gj4LeGDdP-y-yR--', 'test.bea60k')\n",
        "    download_file_from_google_drive('16AMIb6FVltgRR8xv8h7qacDUX8cOQK9d', 'test.bea60k.noise')\n",
        "\n",
        "    download_file_from_google_drive('192g_5oJn4dro5QJ88Dd8-lN_xKE_lLf0', 'test.bea322')\n",
        "    download_file_from_google_drive('1_hka2FOT4FrMvsV3d4Zfi9W8v3oFBRYc', 'test.bea322.noise')\n",
        "\n",
        "    download_file_from_google_drive('1v0tRcNZctvVGqrmjlda_6dH8AfZUzFGO', 'test.bea4660')\n",
        "    download_file_from_google_drive('1EmuKeNgBRzc760R0dSSuZ36xdikQRlIS', 'test.bea4660.noise')\n",
        "\n",
        "    download_file_from_google_drive('1jHR2f3JwnskDphQcaTXr0hLlp60qJxUl', 'test.jfleg')\n",
        "    download_file_from_google_drive('1sccH7dRhyctKAIQXBZEBmUWEiTN_-o6q', 'test.jfleg.noise')\n",
        "\n",
        "    download_file_from_google_drive('1aWHIxu_BrZIeGRLhID3J_od6shXz3jUb', 'train.1blm')\n",
        "    download_file_from_google_drive('16RYImD2esgGwc1nNt3Yf-WR5TU1yQyik', 'train.1blm.noise.prob')\n",
        "    download_file_from_google_drive('11FMI2C-ouwaWesTLjfPCXmqeB6HUQHkK', 'train.1blm.noise.random')\n",
        "    download_file_from_google_drive('1eRpWqSb7sIm3kgtkdfVTru9YKHSRRrdq', 'train.1blm.noise.word')\n",
        "\n",
        "    download_file_from_google_drive('1INTWXWO6i1Swthu5ln7REZjGvPFv2hyQ', 'train.bea40k')\n",
        "    download_file_from_google_drive('1KTeL8oZ30fVI_QuCW879T-CgfeewvSk9', 'train.bea40k.noise')\n",
        "\n",
        "    download_file_from_google_drive('1s6CQ6NlsstCLbLCSEZyP-uvNwx4UwzZT', 'train.moviereviews')\n",
        "    download_file_from_google_drive('1xk3jyTkiVEWDsl-Abhc8XUXiIVp_WXCg', 'valid.moviereviews')\n",
        "\n",
        "\n",
        "    # wo_context\n",
        "    create_paths(\"./wo_context\")\n",
        "\n",
        "    download_file_from_google_drive('1uNHQovF2Z0QPp27i2Sq5abVL_f56iNYK', './wo_context/aspell_big')\n",
        "    download_file_from_google_drive('19eSfVnX-sIdUnaazEaUCsHsFt8p5qoXZ', './wo_context/aspell_big.noise')\n",
        "\n",
        "    download_file_from_google_drive('1XqHN1VnVnVnSR4-wF_iI6VUVPf9S_TP3', './wo_context/aspell_small')\n",
        "    download_file_from_google_drive('1I9jhthL6y52h8uRuwcnRjhtRbNw3mX8V', './wo_context/aspell_small.noise')\n",
        "\n",
        "    download_file_from_google_drive('1ptBfh8UvbAUH7K1TIALDZM8CUL_Yhxty', './wo_context/combined_data')\n",
        "    download_file_from_google_drive('1bSye_TITRUdO4CUIt9i1R2PkCdQD346h', './wo_context/combined_data.noise')\n",
        "\n",
        "    download_file_from_google_drive('1bj9zQqntrVRydBn-YHZ0BcT55Xf37jq-', './wo_context/homophones')\n",
        "    download_file_from_google_drive('1rL_OdqQgr-kL6X_N94epxzpnIzj_L6y6', './wo_context/homophones.noise')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yegdHSd3zq56",
        "outputId": "596832b3-33eb-4027-f2d5-6236d7efb0b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JForBOqptb_G"
      },
      "source": [
        "\n",
        "# 3. Download Checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ggvON9JDc4A"
      },
      "source": [
        "Download the required models of neuspell from the Google Drive link below\n",
        "\n",
        "https://drive.google.com/drive/folders/1jgNpYe4TVSF4mMBVtFh4QfB2GovNPdh7?usp=sharing\n",
        "\n",
        "Extract it and place it in the “neuspell/data/checkpoints” folder.\n",
        "\n",
        "Make sure to replace any existing model files (if any) with the new one.\n",
        "\n",
        "You can find the name of model and checkpoint to download from the drive in the following table\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "| Spell Checker                       | Class               | Checkpoint name             | Disk space (approx.) |\n",
        "|-------------------------------------|---------------------|-----------------------------|----------------------|\n",
        "| ```CNN-LSTM```                      | `CnnlstmChecker`    | 'cnn-lstm-probwordnoise'    | 450 MB               |\n",
        "| ```SC-LSTM```                       | `SclstmChecker`     | 'scrnn-probwordnoise'       | 450 MB               |\n",
        "| ```Nested-LSTM```                   | `NestedlstmChecker` | 'lstm-lstm-probwordnoise'   | 455 MB               |\n",
        "| ```BERT```                          | `BertChecker`       | 'subwordbert-probwordnoise' | 740 MB               |\n",
        "| ```SC-LSTM plus ELMO (at input)```  | `ElmosclstmChecker` | 'elmoscrnn-probwordnoise'   | 840 MB               |\n",
        "| ```SC-LSTM plus BERT (at input)```  | `BertsclstmChecker` | 'bertscrnn-probwordnoise'   | 900 MB               |\n",
        "| ```SC-LSTM plus BERT (at output)``` | `SclstmbertChecker` | 'scrnnbert-probwordnoise'   | 1.19 GB              |\n",
        "| ```SC-LSTM plus ELMO (at output)``` | `SclstmelmoChecker` | 'scrnnelmo-probwordnoise'   | 1.23 GB              |\n",
        "\n",
        "\n",
        "---\n",
        "Note: Each Model will take 5-10mins to upload depending on the size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYf10pL8SU4B"
      },
      "source": [
        "# 4. Implemetation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS5paVhXP9Fg"
      },
      "source": [
        "## List of neural models implemeted:\n",
        "\n",
        "- [```SC-LSTM```](https://drive.google.com/file/d/1OvbkdBXawnefQF1d-tUrd9lxiAH1ULtr/view?usp=sharing)\n",
        "It corrects misspelt words using semi-character represen- tations, fed through a bi-LSTM network. The semi-character representations are a concate- nation of one-hot embeddings for the (i) first, (ii) last, and (iii) bag of internal characters\n",
        "- [```Nested-LSTM```](https://drive.google.com/file/d/19ZhWvBaZqrsP5cGqBJdFPtufdyBqQprI/view?usp=sharing)\n",
        "The model builds word representations by passing its individual characters to a bi-LSTM. These representations are further fed to another bi- LSTM trained to predict the correction\n",
        "- [```CNN-LSTM```](https://drive.google.com/file/d/14XiDY4BJ144fVGE2cfWfwyjnMwBcwhNa/view?usp=sharing)\n",
        "Similar to the previous model, this model builds word- level representations from individual charac- ters using a convolutional network.\n",
        "- [```BERT```](https://huggingface.co/transformers/bertology.html)\n",
        "The model uses a pre-trained transformer network. We aver- age the sub-word representations to obtain the word representations, which are further fed to a classifier to predict its correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxzcb4D3Cg7u"
      },
      "source": [
        "The above mentioned 4 neural models have been implemented and the code has been modifed for the same"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEwIfy1hOpbH"
      },
      "source": [
        "## 4.1 Defining the listed Models in NeuSpell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "e9b28a59b0a844379ab7c2b9cb1b7e22",
            "fdbf7d63b202456b966bfbf9fced4d84",
            "e2a8de42e45a474fb35a60405a3db5f4",
            "fc85ea168655423193b72a1a3b081185",
            "62bbeca5a45a493fac2fc2e5ad646978",
            "7df74a0a26714140b25b1f4f6b5f0a96",
            "fdb1029ea3ae4e8693ea43800e39964a",
            "65d3229d5b7f4262920b2a81ac6f1041",
            "38964e8bc5434b00abed61ac76fc0635",
            "8927fdd488ce4e12a0c77234f3a66b08",
            "93955f9043c14723b4ba1626b1bbb9ea",
            "5bcc0eee9f114fa09641d4eb31f4fe50",
            "eca86ac452bf404598af288b242bb830",
            "bacfb6c6b19a4ef0a1aeb9954e43a1bf",
            "16daad3c524a4bb1b27d7397d995a168",
            "fdd871f3ad2d4bd3a37d574f203842f1",
            "867517bb807d4d18bef22a2ea8b0e4b7",
            "c3c0ccc423644429a2d5f5cd486d77f5",
            "6588358fdc3f4fb0844207f05ec2b66c",
            "cbf34fb2c2be4998894c2413777dcd97",
            "fc5401ee05314946bb02d23e6022ca60",
            "ac0dd98e18394c428682d29a9edc1d2f",
            "93f85af45ad54af9b85a7091711d5b07",
            "b38bc2635b3d440098b299be3a7a6c3a",
            "da5069e2643b483c9eb4a395599747a1",
            "ed3b02edff5642f6817fe27f25fc3c02",
            "143d82f3b37c4af2b3daf6bc641a120e",
            "005c43efd7744d6d88f5ed3afe6c5264",
            "15ce3d4a9e2346b6943dc5a092f5c6b9",
            "2b88687aba0e4c69aaff74da9ddffe9a",
            "f7d22abb73e14876bbdde13e7374bb85",
            "644eab0db4fb4d70be915551d5e6a912",
            "449be78d7b094bdd8abc81290fb26d0c",
            "21e3b9acb5104aa09c164ef3f8d86ab8",
            "2c7c9aef5835448fbab71380d5fff448",
            "b871755546f347329a08e25b25bc846d",
            "d9a3dad4c264437d9ba109cdfa7d7be2",
            "639c305bf781491f8aa8ffaa2126348c",
            "43df29b215924373b55e1a79d378a38c",
            "69da365bcb2c4760a2bdb3cbe752afbc",
            "e5aeb6635690468bb88685b815aad3d4",
            "377c24f38a534b95b3f5b030a58479e2",
            "7b735684c5f54841bb52099fc52a62ec",
            "a252a7131a2a407fa060b0257025aab1"
          ]
        },
        "id": "LdjFJs-kNrzN",
        "outputId": "3a55c0e2-3dbd-4c26-f5f5-c66a96adfff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data folder is set to `/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data` script\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9b28a59b0a844379ab7c2b9cb1b7e22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bcc0eee9f114fa09641d4eb31f4fe50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93f85af45ad54af9b85a7091711d5b07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21e3b9acb5104aa09c164ef3f8d86ab8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from neuspell.util import is_module_available, get_module_or_attr\n",
        "from neuspell.commons import ALLENNLP_ELMO_PRETRAINED_FOLDER\n",
        "\n",
        "DEFAULT_BERT_PRETRAINED_NAME_OR_PATH = \"bert-base-cased\"\n",
        "\n",
        "\n",
        "def get_pretrained_bert(pretrained_name_or_path=None):\n",
        "    pretrained_name_or_path = pretrained_name_or_path or DEFAULT_BERT_PRETRAINED_NAME_OR_PATH\n",
        "    return transformers.AutoModel.from_pretrained(pretrained_name_or_path)\n",
        "\n",
        "\n",
        "def get_pretrained_elmo(elmo_options_file=None, elmo_weights_file=None):\n",
        "    if not is_module_available(\"allennlp\"):\n",
        "        raise ImportError(\n",
        "            \"install `allennlp` by running `pip install -r extras-requirements.txt`. See `README.md` for more info.\")\n",
        "\n",
        "    Elmo = get_module_or_attr(\"allennlp.modules.elmo\", \"Elmo\")\n",
        "\n",
        "    local_options_file, local_weights_file = None, None\n",
        "    if os.path.exists(ALLENNLP_ELMO_PRETRAINED_FOLDER):\n",
        "        local_options_file = os.path.join(ALLENNLP_ELMO_PRETRAINED_FOLDER,\n",
        "                                          \"elmo_2x4096_512_2048cnn_2xhighway_options.json\")\n",
        "        local_weights_file = os.path.join(ALLENNLP_ELMO_PRETRAINED_FOLDER,\n",
        "                                          \"elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\")\n",
        "\n",
        "    options_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
        "    weights_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
        "\n",
        "    elmo_options_file = elmo_options_file or local_options_file or options_file  # or os.environ.get('ELMO_OPTIONS_FILE_PATH', None)\n",
        "    elmo_weights_file = elmo_weights_file or local_weights_file or weights_file  # or os.environ.get('ELMO_WEIGHTS_FILE_PATH', None)\n",
        "    # neuspell.seq_modeling.models.get_pretrained_elmo()\n",
        "    return Elmo(elmo_options_file, elmo_weights_file, 1)  # 1 for setting device=\"cuda:0\" else 0\n",
        "\n",
        "\n",
        "#################################################\n",
        "# CharCNNWordLSTMModel(CharCNNModel)\n",
        "#################################################\n",
        "\n",
        "class CharCNNModel(nn.Module):\n",
        "    def __init__(self, nembs, embdim, padding_idx, filterlens, nfilters):\n",
        "        super(CharCNNModel, self).__init__()\n",
        "\n",
        "        # Embeddings\n",
        "        self.embeddings = nn.Embedding(nembs, embdim, padding_idx=padding_idx)\n",
        "        # torch.nn.init.normal_(self.embeddings.weight.data, std=1.0)\n",
        "        self.embeddings.weight.requires_grad = True\n",
        "\n",
        "        # Unsqueeze [BS, MAXSEQ, EMDDIM] as [BS, 1, MAXSEQ, EMDDIM] and send as input\n",
        "        self.convmodule = nn.ModuleList()\n",
        "        for length, n in zip(filterlens, nfilters):\n",
        "            self.convmodule.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(1, n, (length, embdim), padding=(length - 1, 0), dilation=1, bias=True,\n",
        "                              padding_mode='zeros'),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "            )\n",
        "        # each conv outputs [BS, nfilters, MAXSEQ, 1]\n",
        "\n",
        "    def forward(self, batch_tensor):\n",
        "        batch_size = len(batch_tensor)\n",
        "\n",
        "        # [BS, max_seq_len]->[BS, max_seq_len, emb_dim]\n",
        "        embs = self.embeddings(batch_tensor)\n",
        "\n",
        "        # [BS, max_seq_len, emb_dim]->[BS, 1, max_seq_len, emb_dim]\n",
        "        embs_unsqueezed = torch.unsqueeze(embs, dim=1)\n",
        "\n",
        "        # [BS, 1, max_seq_len, emb_dim]->[BS, out_channels, max_seq_len, 1]->[BS, out_channels, max_seq_len]\n",
        "        conv_outputs = [conv(embs_unsqueezed).squeeze(3) for conv in self.convmodule]\n",
        "\n",
        "        # [BS, out_channels, max_seq_len]->[BS, out_channels]\n",
        "        maxpool_conv_outputs = [F.max_pool1d(out, out.size(2)).squeeze(2) for out in conv_outputs]\n",
        "\n",
        "        # cat( [BS, out_channels] )->[BS, sum(nfilters)]\n",
        "        source_encodings = torch.cat(maxpool_conv_outputs, dim=1)\n",
        "        return source_encodings\n",
        "\n",
        "\n",
        "class CharCNNWordLSTMModel(nn.Module):\n",
        "    def __init__(self, nchars, char_emb_dim, char_padding_idx, padding_idx, output_dim):\n",
        "        super(CharCNNWordLSTMModel, self).__init__()\n",
        "\n",
        "        # cnn module\n",
        "        # takes in a list[pad_sequence] with each pad_sequence of dim: [BS][nwords,max_nchars]\n",
        "        # runs a for loop to obtain list[tensor] with each tensor of dim: [BS][nwords,sum(nfilters)]\n",
        "        # then use rnn.pad_sequence(.) to obtain the dim: [BS, max_nwords, sum(nfilters)]\n",
        "        nfilters, filtersizes = [50, 100, 100, 100], [2, 3, 4, 5]\n",
        "        self.cnnmodule = CharCNNModel(nchars, char_emb_dim, char_padding_idx, filtersizes, nfilters)\n",
        "        self.cnnmodule_outdim = sum(nfilters)\n",
        "\n",
        "        # lstm module\n",
        "        # expected  input dim: [BS,max_nwords,*] and batch_lengths as [BS] for pack_padded_sequence\n",
        "        bidirectional, hidden_size, nlayers = True, 512, 2\n",
        "        self.lstmmodule = nn.LSTM(self.cnnmodule_outdim, hidden_size, nlayers,\n",
        "                                  batch_first=True, dropout=0.3, bidirectional=bidirectional)\n",
        "        self.lstmmodule_outdim = hidden_size * 2 if bidirectional else hidden_size\n",
        "\n",
        "        # output module\n",
        "        assert output_dim > 0\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        self.dense = nn.Linear(self.lstmmodule_outdim, output_dim)\n",
        "\n",
        "        # loss\n",
        "        # See https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=padding_idx)\n",
        "\n",
        "    def forward(self,\n",
        "                batch_idxs: \"list[pad_sequence]\",\n",
        "                batch_lengths: \"tensor\",\n",
        "                aux_word_embs: \"tensor\" = None,\n",
        "                targets: \"tensor\" = None,\n",
        "                topk=1):\n",
        "\n",
        "        batch_size = len(batch_idxs)\n",
        "\n",
        "        # cnn\n",
        "        cnn_encodings = [self.cnnmodule(pad_sequence_) for pad_sequence_ in batch_idxs]\n",
        "        cnn_encodings = pad_sequence(cnn_encodings, batch_first=True, padding_value=0)\n",
        "\n",
        "        # concat aux_embs\n",
        "        # if not None, the expected dim for aux_word_embs: [BS,max_nwords,*]\n",
        "        intermediate_encodings = cnn_encodings\n",
        "        if aux_word_embs is not None:\n",
        "            intermediate_encodings = torch.cat((intermediate_encodings, aux_word_embs), dim=2)\n",
        "\n",
        "        # lstm\n",
        "        # dim: [BS,max_nwords,*]->[BS,max_nwords,self.lstmmodule_outdim]\n",
        "        intermediate_encodings = pack_padded_sequence(intermediate_encodings, batch_lengths,\n",
        "                                                      batch_first=True, enforce_sorted=False)\n",
        "        lstm_encodings, (last_hidden_states, last_cell_states) = self.lstmmodule(intermediate_encodings)\n",
        "        lstm_encodings, _ = pad_packed_sequence(lstm_encodings, batch_first=True, padding_value=0)\n",
        "\n",
        "        # dense\n",
        "        # [BS,max_nwords,self.lstmmodule_outdim]->[BS,max_nwords,output_dim]\n",
        "        logits = self.dense(self.dropout(lstm_encodings))\n",
        "\n",
        "        # loss\n",
        "        if targets is not None:\n",
        "            assert len(targets) == batch_size  # targets:[[BS,max_nwords]\n",
        "            logits_permuted = logits.permute(0, 2, 1)  # logits: [BS,output_dim,max_nwords]\n",
        "            loss = self.criterion(logits_permuted, targets)\n",
        "\n",
        "        # eval preds\n",
        "        if not self.training:\n",
        "            probs = F.softmax(logits, dim=-1)  # [BS,max_nwords,output_dim]\n",
        "            if topk > 1:\n",
        "                topk_values, topk_inds = \\\n",
        "                    torch.topk(probs, topk, dim=-1, largest=True,\n",
        "                               sorted=True)  # -> (Tensor, LongTensor) of [BS,max_nwords,topk]\n",
        "            elif topk == 1:\n",
        "                topk_inds = torch.argmax(probs, dim=-1)  # [BS,max_nwords]\n",
        "\n",
        "            # Note that for those positions with padded_idx,\n",
        "            #   the arg_max_prob above computes a index because\n",
        "            #   the bias term leads to non-uniform values in those positions\n",
        "\n",
        "            return loss.cpu().detach().numpy(), topk_inds.cpu().detach().numpy()\n",
        "        return loss\n",
        "\n",
        "\n",
        "#################################################\n",
        "# CharLSTMWordLSTMModel(CharLSTMModel)\n",
        "#################################################\n",
        "\n",
        "class CharLSTMModel(nn.Module):\n",
        "    def __init__(self, nembs, embdim, padding_idx, hidden_size, num_layers, bidirectional, output_combination):\n",
        "        super(CharLSTMModel, self).__init__()\n",
        "\n",
        "        # Embeddings\n",
        "        self.embeddings = nn.Embedding(nembs, embdim, padding_idx=padding_idx)\n",
        "        # torch.nn.init.normal_(self.embeddings.weight.data, std=1.0)\n",
        "        self.embeddings.weight.requires_grad = True\n",
        "\n",
        "        # lstm module\n",
        "        # expected input dim: [BS,max_nwords,*] and batch_lengths as [BS] for pack_padded_sequence\n",
        "        self.lstmmodule = nn.LSTM(embdim, hidden_size, num_layers, batch_first=True, dropout=0.3,\n",
        "                                  bidirectional=bidirectional)\n",
        "        self.lstmmodule_outdim = hidden_size * 2 if bidirectional else hidden_size\n",
        "\n",
        "        # output\n",
        "        assert output_combination in [\"end\", \"max\", \"mean\"], print(\n",
        "            'invalid output_combination; required one of {\"end\",\"max\",\"mean\"}')\n",
        "        self.output_combination = output_combination\n",
        "\n",
        "    def forward(self, batch_tensor, batch_lengths):\n",
        "\n",
        "        batch_size = len(batch_tensor)\n",
        "        # print(\"************ stage 2\")\n",
        "\n",
        "        # [BS, max_seq_len]->[BS, max_seq_len, emb_dim]\n",
        "        embs = self.embeddings(batch_tensor)\n",
        "\n",
        "        # lstm\n",
        "        # dim: [BS,max_nwords,*]->[BS,max_nwords,self.lstmmodule_outdim]\n",
        "        embs_packed = pack_padded_sequence(embs, batch_lengths, batch_first=True, enforce_sorted=False)\n",
        "        lstm_encodings, (last_hidden_states, last_cell_states) = self.lstmmodule(embs_packed)\n",
        "        lstm_encodings, _ = pad_packed_sequence(lstm_encodings, batch_first=True, padding_value=0)\n",
        "\n",
        "        # [BS, max_seq_len, self.lstmmodule_outdim]->[BS, self.lstmmodule_outdim]\n",
        "        if self.output_combination == \"end\":\n",
        "            last_seq_idxs = torch.LongTensor([x - 1 for x in batch_lengths])\n",
        "            source_encodings = lstm_encodings[range(lstm_encodings.shape[0]), last_seq_idxs, :]\n",
        "        elif self.output_combination == \"max\":\n",
        "            source_encodings, _ = torch.max(lstm_encodings, dim=1)\n",
        "        elif self.output_combination == \"mean\":\n",
        "            sum_ = torch.sum(lstm_encodings, dim=1)\n",
        "            lens_ = batch_lengths.unsqueeze(dim=1).expand(batch_size, self.lstmmodule_outdim)\n",
        "            assert sum_.size() == lens_.size()\n",
        "            source_encodings = torch.div(sum_, lens_)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        return source_encodings\n",
        "\n",
        "\n",
        "class CharLSTMWordLSTMModel(nn.Module):\n",
        "    def __init__(self, nchars, char_emb_dim, char_padding_idx, padding_idx, output_dim):\n",
        "        super(CharLSTMWordLSTMModel, self).__init__()\n",
        "\n",
        "        # charlstm module\n",
        "        # takes in a list[pad_sequence] with each pad_sequence of dim: [BS][nwords,max_nchars]\n",
        "        # runs a for loop to obtain list[tensor] with each tensor of dim: [BS][nwords,charlstm_outputdim]\n",
        "        # then use rnn.pad_sequence(.) to obtain the dim: [BS, max_nwords, charlstm_outputdim]\n",
        "        hidden_size, num_layers, bidirectional, output_combination = 256, 1, True, \"end\"\n",
        "        self.charlstmmodule = CharLSTMModel(nchars, char_emb_dim, char_padding_idx, hidden_size, num_layers,\n",
        "                                            bidirectional, output_combination)\n",
        "        self.charlstmmodule_outdim = self.charlstmmodule.lstmmodule_outdim\n",
        "\n",
        "        # lstm module\n",
        "        # expected  input dim: [BS,max_nwords,*] and batch_lengths as [BS] for pack_padded_sequence\n",
        "        bidirectional, hidden_size, nlayers = True, 512, 2\n",
        "        self.lstmmodule = nn.LSTM(self.charlstmmodule_outdim, hidden_size, nlayers,\n",
        "                                  batch_first=True, dropout=0.3, bidirectional=bidirectional)\n",
        "        self.lstmmodule_outdim = hidden_size * 2 if bidirectional else hidden_size\n",
        "\n",
        "        # output module\n",
        "        assert output_dim > 0\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        self.dense = nn.Linear(self.lstmmodule_outdim, output_dim)\n",
        "\n",
        "        # loss\n",
        "        # See https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=padding_idx)\n",
        "\n",
        "    def forward(self,\n",
        "                batch_idxs: \"list[pad_sequence]\",\n",
        "                batch_char_lengths: \"list[tensor]\",\n",
        "                batch_lengths: \"tensor\",\n",
        "                aux_word_embs: \"tensor\" = None,\n",
        "                targets: \"tensor\" = None,\n",
        "                topk=1):\n",
        "\n",
        "        batch_size = len(batch_idxs)\n",
        "        # print(\"************ stage 1\")\n",
        "\n",
        "        # charlstm\n",
        "        charlstm_encodings = [self.charlstmmodule(pad_sequence_, lens) for pad_sequence_, lens in\n",
        "                              zip(batch_idxs, batch_char_lengths)]\n",
        "        charlstm_encodings = pad_sequence(charlstm_encodings, batch_first=True, padding_value=0)\n",
        "\n",
        "        # concat aux_embs\n",
        "        # if not None, the expected dim for aux_word_embs: [BS,max_nwords,*]\n",
        "        intermediate_encodings = charlstm_encodings\n",
        "        if aux_word_embs is not None:\n",
        "            intermediate_encodings = torch.cat((intermediate_encodings, aux_word_embs), dim=2)\n",
        "\n",
        "        # lstm\n",
        "        # dim: [BS,max_nwords,*]->[BS,max_nwords,self.lstmmodule_outdim]\n",
        "        intermediate_encodings = pack_padded_sequence(intermediate_encodings, batch_lengths,\n",
        "                                                      batch_first=True, enforce_sorted=False)\n",
        "        lstm_encodings, (last_hidden_states, last_cell_states) = self.lstmmodule(intermediate_encodings)\n",
        "        lstm_encodings, _ = pad_packed_sequence(lstm_encodings, batch_first=True, padding_value=0)\n",
        "\n",
        "        # dense\n",
        "        # [BS,max_nwords,self.lstmmodule_outdim]->[BS,max_nwords,output_dim]\n",
        "        logits = self.dense(self.dropout(lstm_encodings))\n",
        "\n",
        "        # loss\n",
        "        if targets is not None:\n",
        "            assert len(targets) == batch_size  # targets:[[BS,max_nwords]\n",
        "            logits_permuted = logits.permute(0, 2, 1)  # logits: [BS,output_dim,max_nwords]\n",
        "            loss = self.criterion(logits_permuted, targets)\n",
        "\n",
        "        # eval preds\n",
        "        if not self.training:\n",
        "            probs = F.softmax(logits, dim=-1)  # [BS,max_nwords,output_dim]\n",
        "            if topk > 1:\n",
        "                topk_values, topk_inds = \\\n",
        "                    torch.topk(probs, topk, dim=-1, largest=True,\n",
        "                               sorted=True)  # -> (Tensor, LongTensor) of [BS,max_nwords,topk]\n",
        "            elif topk == 1:\n",
        "                topk_inds = torch.argmax(probs, dim=-1)  # [BS,max_nwords]\n",
        "\n",
        "            # Note that for those positions with padded_idx,\n",
        "            #   the arg_max_prob above computes a index because\n",
        "            #   the bias term leads to non-uniform values in those positions\n",
        "\n",
        "            return loss.cpu().detach().numpy(), topk_inds.cpu().detach().numpy()\n",
        "        return loss\n",
        "\n",
        "\n",
        "#################################################\n",
        "# SCLSTM\n",
        "#################################################\n",
        "\n",
        "class SCLSTM(nn.Module):\n",
        "    def __init__(self, screp_dim, padding_idx, output_dim):\n",
        "        super(SCLSTM, self).__init__()\n",
        "        # lstm module\n",
        "        # expected  input dim: [BS,max_nwords,*] and batch_lengths as [BS] for pack_padded_sequence\n",
        "        bidirectional, hidden_size, nlayers = True, 512, 2\n",
        "        self.lstmmodule = nn.LSTM(screp_dim, hidden_size, nlayers,\n",
        "                                  batch_first=True, dropout=0.4, bidirectional=bidirectional)  # 0.3 or 0.4\n",
        "        self.lstmmodule_outdim = hidden_size * 2 if bidirectional else hidden_size\n",
        "\n",
        "        # output module\n",
        "        assert output_dim > 0\n",
        "        self.dropout = nn.Dropout(p=0.5)  # 0.4 or 0.5\n",
        "        self.dense = nn.Linear(self.lstmmodule_outdim, output_dim)\n",
        "\n",
        "        # loss\n",
        "        # See https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=padding_idx)\n",
        "\n",
        "    def forward(self,\n",
        "                batch_screps: \"list[pad_sequence]\",\n",
        "                batch_lengths: \"tensor\",\n",
        "                aux_word_embs: \"tensor\" = None,\n",
        "                targets: \"tensor\" = None,\n",
        "                topk=1):\n",
        "\n",
        "        # cnn\n",
        "        batch_size = len(batch_screps)\n",
        "        batch_screps = pad_sequence(batch_screps, batch_first=True, padding_value=0)\n",
        "\n",
        "        # concat aux_embs\n",
        "        # if not None, the expected dim for aux_word_embs: [BS,max_nwords,*]\n",
        "        intermediate_encodings = batch_screps\n",
        "        if aux_word_embs is not None:\n",
        "            intermediate_encodings = torch.cat((intermediate_encodings, aux_word_embs), dim=2)\n",
        "\n",
        "        # lstm\n",
        "        # dim: [BS,max_nwords,*]->[BS,max_nwords,self.lstmmodule_outdim]\n",
        "        intermediate_encodings = pack_padded_sequence(intermediate_encodings, batch_lengths,\n",
        "                                                      batch_first=True, enforce_sorted=False)\n",
        "        lstm_encodings, (last_hidden_states, last_cell_states) = self.lstmmodule(intermediate_encodings)\n",
        "        lstm_encodings, _ = pad_packed_sequence(lstm_encodings, batch_first=True, padding_value=0)\n",
        "\n",
        "        # dense\n",
        "        # [BS,max_nwords,self.lstmmodule_outdim]->[BS,max_nwords,output_dim]\n",
        "        logits = self.dense(self.dropout(lstm_encodings))\n",
        "\n",
        "        # loss\n",
        "        if targets is not None:\n",
        "            assert len(targets) == batch_size  # targets:[[BS,max_nwords]\n",
        "            logits_permuted = logits.permute(0, 2, 1)  # logits: [BS,output_dim,max_nwords]\n",
        "            loss = self.criterion(logits_permuted, targets)\n",
        "\n",
        "        # eval preds\n",
        "        if not self.training:\n",
        "            probs = F.softmax(logits, dim=-1)  # [BS,max_nwords,output_dim]\n",
        "            if topk > 1:\n",
        "                topk_values, topk_inds = \\\n",
        "                    torch.topk(probs, topk, dim=-1, largest=True,\n",
        "                               sorted=True)  # -> (Tensor, LongTensor) of [BS,max_nwords,topk]\n",
        "            elif topk == 1:\n",
        "                topk_inds = torch.argmax(probs, dim=-1)  # [BS,max_nwords]\n",
        "\n",
        "            # Note that for those positions with padded_idx,\n",
        "            #   the arg_max_prob above computes a index because\n",
        "            #   the bias term leads to non-uniform values in those positions\n",
        "\n",
        "            return loss.cpu().detach().numpy(), topk_inds.cpu().detach().numpy()\n",
        "        return loss\n",
        "\n",
        "\n",
        "#################################################\n",
        "# SubwordElmo\n",
        "#################################################\n",
        "\n",
        "class SubwordElmo(nn.Module):\n",
        "    def __init__(self, screp_dim, padding_idx, output_dim):\n",
        "        super(SubwordElmo, self).__init__()\n",
        "\n",
        "        self.elmo = get_pretrained_elmo()\n",
        "        self.elmomodule_outdim = 1024\n",
        "\n",
        "        # output module\n",
        "        assert output_dim > 0\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        self.dense = nn.Linear(self.elmomodule_outdim, output_dim)\n",
        "\n",
        "        # loss\n",
        "        # See https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=padding_idx)\n",
        "\n",
        "    def forward(self,\n",
        "                batch_elmo_inp: \"tensor\",\n",
        "                aux_word_embs: \"tensor\" = None,\n",
        "                targets: \"tensor\" = None,\n",
        "                topk=1,\n",
        "                beam_search=False):\n",
        "\n",
        "        # cnn\n",
        "        batch_size = len(batch_elmo_inp)\n",
        "\n",
        "        # elmo\n",
        "        elmo_encodings = self.elmo(batch_elmo_inp)['elmo_representations'][0]  # BS X max_nwords x 1024\n",
        "\n",
        "        # concat aux_embs\n",
        "        # if not None, the expected dim for aux_word_embs: [BS,max_nwords,*]\n",
        "        intermediate_encodings = elmo_encodings\n",
        "        if aux_word_embs is not None:\n",
        "            intermediate_encodings = torch.cat((intermediate_encodings, aux_word_embs), dim=2)\n",
        "\n",
        "        # dense\n",
        "        # [BS,max_nwords,self.lstmmodule_outdim]->[BS,max_nwords,output_dim]\n",
        "        logits = self.dense(self.dropout(intermediate_encodings))\n",
        "\n",
        "        # loss\n",
        "        if targets is not None:\n",
        "            assert len(targets) == batch_size  # targets:[[BS,max_nwords]\n",
        "            logits_permuted = logits.permute(0, 2, 1)  # logits: [BS,output_dim,max_nwords]\n",
        "            loss = self.criterion(logits_permuted, targets)\n",
        "\n",
        "        # eval preds\n",
        "        if not self.training:\n",
        "            probs = F.softmax(logits, dim=-1)  # [BS,max_nwords,output_dim]\n",
        "\n",
        "            if not beam_search:\n",
        "                if topk > 1:\n",
        "                    topk_probs, topk_inds = \\\n",
        "                        torch.topk(probs, topk, dim=-1, largest=True,\n",
        "                                   sorted=True)  # -> (Tensor, LongTensor) of [BS,max_nwords,topk]\n",
        "                elif topk == 1:\n",
        "                    topk_inds = torch.argmax(probs, dim=-1)  # [BS,max_nwords]\n",
        "                else:\n",
        "                    raise Exception(\"topk can be one of a value>=1\")\n",
        "\n",
        "                # Note that for those positions with padded_idx,\n",
        "                #   the arg_max_prob above computes a index because\n",
        "                #   the bias term leads to non-uniform values in those positions\n",
        "\n",
        "                return loss.cpu().detach().numpy(), topk_inds.cpu().detach().numpy()\n",
        "\n",
        "            else:\n",
        "                topk_probs, topk_inds = \\\n",
        "                    torch.topk(probs, topk, dim=-1, largest=True,\n",
        "                               sorted=True)  # -> (Tensor, LongTensor) of [BS,max_nwords,topk]\n",
        "                return loss.cpu().detach().numpy(), topk_inds.cpu().detach().numpy(), topk_probs.cpu().detach().numpy()\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "#################################################\n",
        "# SubwordBert\n",
        "#################################################\n",
        "\n",
        "class SubwordBert(nn.Module):\n",
        "    def __init__(self, padding_idx, output_dim, bert_pretrained_name_or_path=None, freeze_bert=False):\n",
        "        super(SubwordBert, self).__init__()\n",
        "\n",
        "        self.bert_dropout = torch.nn.Dropout(0.2)\n",
        "        self.bert_model = get_pretrained_bert(bert_pretrained_name_or_path)\n",
        "        self.bertmodule_outdim = self.bert_model.config.hidden_size\n",
        "        if freeze_bert:\n",
        "            # Uncomment to freeze BERT layers\n",
        "            for param in self.bert_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # output module\n",
        "        assert output_dim > 0\n",
        "        # self.dropout = nn.Dropout(p=0.4)\n",
        "        self.dense = nn.Linear(self.bertmodule_outdim, output_dim)\n",
        "\n",
        "        # loss\n",
        "        # See https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=padding_idx)\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def get_merged_encodings(self, bert_seq_encodings, seq_splits, mode='avg'):\n",
        "        bert_seq_encodings = bert_seq_encodings[:sum(seq_splits) + 2, :]  # 2 for [CLS] and [SEP]\n",
        "        bert_seq_encodings = bert_seq_encodings[1:-1, :]\n",
        "        # a tuple of tensors\n",
        "        split_encoding = torch.split(bert_seq_encodings, seq_splits, dim=0)\n",
        "        batched_encodings = pad_sequence(split_encoding, batch_first=True, padding_value=0)\n",
        "        if mode == 'avg':\n",
        "            seq_splits = torch.tensor(seq_splits).reshape(-1, 1).to(self.device)\n",
        "            out = torch.div(torch.sum(batched_encodings, dim=1), seq_splits)\n",
        "        elif mode == \"add\":\n",
        "            out = torch.sum(batched_encodings, dim=1)\n",
        "        else:\n",
        "            raise Exception(\"Not Implemented\")\n",
        "        return out\n",
        "\n",
        "    def forward(self,\n",
        "                batch_bert_dict: \"{'input_ids':tensor, 'attention_mask':tensor, 'token_type_ids':tensor}\",\n",
        "                batch_splits: \"list[list[int]]\",\n",
        "                aux_word_embs: \"tensor\" = None,\n",
        "                targets: \"tensor\" = None,\n",
        "                topk=1):\n",
        "\n",
        "        # cnn\n",
        "        batch_size = len(batch_splits)\n",
        "\n",
        "        # bert\n",
        "        # BS X max_nsubwords x self.bertmodule_outdim\n",
        "        bert_encodings = self.bert_model(**batch_bert_dict, return_dict=False)[0]\n",
        "        bert_encodings = self.bert_dropout(bert_encodings)\n",
        "        # BS X max_nwords x self.bertmodule_outdim\n",
        "        bert_merged_encodings = pad_sequence(\n",
        "            [self.get_merged_encodings(bert_seq_encodings, seq_splits, mode='avg') \\\n",
        "             for bert_seq_encodings, seq_splits in zip(bert_encodings, batch_splits)],\n",
        "            batch_first=True,\n",
        "            padding_value=0\n",
        "        )\n",
        "\n",
        "        # concat aux_embs\n",
        "        # if not None, the expected dim for aux_word_embs: [BS,max_nwords,*]\n",
        "        intermediate_encodings = bert_merged_encodings\n",
        "        if aux_word_embs is not None:\n",
        "            intermediate_encodings = torch.cat((intermediate_encodings, aux_word_embs), dim=2)\n",
        "\n",
        "        # dense\n",
        "        # [BS,max_nwords,*] or [BS,max_nwords,self.bertmodule_outdim]->[BS,max_nwords,output_dim]\n",
        "        # logits = self.dense(self.dropout(intermediate_encodings))\n",
        "        logits = self.dense(intermediate_encodings)\n",
        "\n",
        "        # loss\n",
        "        if targets is not None:\n",
        "            assert len(targets) == batch_size  # targets:[[BS,max_nwords]\n",
        "            logits_permuted = logits.permute(0, 2, 1)  # logits: [BS,output_dim,max_nwords]\n",
        "            loss = self.criterion(logits_permuted, targets)\n",
        "\n",
        "        # eval preds\n",
        "        if not self.training:\n",
        "            probs = F.softmax(logits, dim=-1)  # [BS,max_nwords,output_dim]\n",
        "            if topk > 1:\n",
        "                topk_values, topk_inds = \\\n",
        "                    torch.topk(probs, topk, dim=-1, largest=True,\n",
        "                               sorted=True)  # -> (Tensor, LongTensor) of [BS,max_nwords,topk]\n",
        "            elif topk == 1:\n",
        "                topk_inds = torch.argmax(probs, dim=-1)  # [BS,max_nwords]\n",
        "\n",
        "            # Note that for those positions with padded_idx,\n",
        "            #   the arg_max_prob above computes a index because\n",
        "            #   the bias term leads to non-uniform values in those positions\n",
        "\n",
        "            return loss.cpu().detach().numpy(), topk_inds.cpu().detach().numpy()\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LywJsjgNhwmF"
      },
      "source": [
        "## 4.2 Installing Correctors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UjWUgNYRkwm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "\n",
        "from neuspell.commons import DEFAULT_DATA_PATH\n",
        "from neuspell.seq_modeling.downloads import download_pretrained_model\n",
        "from neuspell.seq_modeling.helpers import load_vocab_dict, get_model_nparams\n",
        "from neuspell.util import is_module_available\n",
        "\n",
        "\n",
        "def get_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size = os.path.getsize(\"temp.p\") / 1e6\n",
        "    os.remove('temp.p')\n",
        "    return size\n",
        "\n",
        "\n",
        "class Corrector(ABC):\n",
        "    DEFAULT_CHECKPOINT_PATH = {\n",
        "        \"cnn-lstm-probwordnoise\": f\"{DEFAULT_DATA_PATH}/checkpoints/cnn-lstm-probwordnoise\",\n",
        "        \"lstm-lstm-probwordnoise\": f\"{DEFAULT_DATA_PATH}/checkpoints/lstm-lstm-probwordnoise\",\n",
        "        \"scrnn-probwordnoise\": f\"{DEFAULT_DATA_PATH}/checkpoints/scrnn-probwordnoise\",\n",
        "        \"subwordbert-probwordnoise\": f\"{DEFAULT_DATA_PATH}/checkpoints/subwordbert-probwordnoise\",\n",
        "    }\n",
        "    if is_module_available(\"allennlp\"):\n",
        "        DEFAULT_CHECKPOINT_PATH.update({\n",
        "            \"elmoscrnn-probwordnoise\": f\"{DEFAULT_DATA_PATH}/checkpoints/elmoscrnn-probwordnoise\",\n",
        "            \"scrnnelmo-probwordnoise\": f\"{DEFAULT_DATA_PATH}/checkpoints/scrnnelmo-probwordnoise\",\n",
        "        })\n",
        "\n",
        "    # TODO: deprecated usage; should be reoved in next versions\n",
        "    DEFAULT_CHECKERNAME_TO_NAME_MAPPING = {\n",
        "        \"CnnlstmChecker\": \"cnn-lstm-probwordnoise\",\n",
        "        \"NestedlstmChecker\": \"lstm-lstm-probwordnoise\",\n",
        "        \"SclstmChecker\": \"scrnn-probwordnoise\",\n",
        "        \"BertChecker\": \"subwordbert-probwordnoise\",\n",
        "    }\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "\n",
        "        self._default_name = kwargs.get(\"name\", None)\n",
        "        self.tokenize = kwargs.get(\"tokenize\", True)\n",
        "        self.device = kwargs.get(\"device\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.device = \"cuda\" if self.device == \"gpu\" else self.device\n",
        "\n",
        "        self.ckpt_path, self.vocab_path, self.weights_path = None, None, None\n",
        "        self.model, self.vocab = None, None\n",
        "\n",
        "        if not self._default_name:\n",
        "\n",
        "            try:\n",
        "                self._default_name = Corrector.DEFAULT_CHECKERNAME_TO_NAME_MAPPING[self.__class__.__name__]\n",
        "            except KeyError as e:\n",
        "                msg = f\"Unable to resolve checker name {self.__class__.__name__} \" \\\n",
        "                      f\"from list of known names {[*NAME_TO_CHECKER_MAPPINGS.keys()]}\"\n",
        "                raise ModuleNotFoundError(msg) from e\n",
        "\n",
        "        if kwargs.get(\"pretrained\", False):\n",
        "            self.from_pretrained(ckpt_path=self.ckpt_path)\n",
        "\n",
        "    def is_model_ready(self):\n",
        "        assert not (self.model is None or self.vocab is None), print(\"model & vocab must be loaded first\")\n",
        "\n",
        "    @property\n",
        "    def get_device(self):\n",
        "        return self.device\n",
        "\n",
        "    def set_device(self, device='cpu'):\n",
        "        prev_device = self.device\n",
        "        device = \"cuda\" if ((device == \"gpu\" or device == \"cuda\") and torch.cuda.is_available()) else \"cpu\"\n",
        "        if not (prev_device == device):\n",
        "            # use .to() if moving from cpu or gpu, and for reverse, use map_location\n",
        "            # https://tinyurl.com/y57pcjvd\n",
        "            # https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html\n",
        "            if self.model is not None:\n",
        "                try:\n",
        "                    self.model.to(device)\n",
        "                except Exception as e:\n",
        "                    try:\n",
        "                        self.from_pretrained(self.ckpt_path, vocab=self.vocab_path)\n",
        "                    except Exception as e:\n",
        "                        msg = f\"Unable to move model from {prev_device} to {device}. \" \\\n",
        "                              f\"Please load a new instance with argument `device={device}. \"\n",
        "                        raise Exception(msg)\n",
        "            self.device = device\n",
        "            print(f\"model set to work on {device}\")\n",
        "        return\n",
        "\n",
        "    def correct(self, x):\n",
        "        return self.correct_string(x)\n",
        "\n",
        "    def correct_string(self, mystring: str, return_all=False) -> str:\n",
        "        x = self.correct_strings([mystring], return_all=return_all)\n",
        "        if return_all:\n",
        "            return x[0][0], x[1][0]\n",
        "        else:\n",
        "            return x[0]\n",
        "\n",
        "    def correct_from_file(self, src, dest=\"./clean_version.txt\"):\n",
        "        self.is_model_ready()\n",
        "        x = [line.strip() for line in open(src, 'r')]\n",
        "        y = self.correct_strings(x)\n",
        "        print(f\"saving results at: {dest}\")\n",
        "        opfile = open(dest, 'w')\n",
        "        for line in y:\n",
        "            opfile.write(line + \"\\n\")\n",
        "        opfile.close()\n",
        "        return\n",
        "\n",
        "    def _from_pretrained(self, ckpt_path=None, vocab_path=None):\n",
        "\n",
        "        if ckpt_path:\n",
        "            self._default_name = os.path.split(ckpt_path)[-1]\n",
        "            self.ckpt_path = ckpt_path\n",
        "        else:\n",
        "            # self._default_name is kept default\n",
        "            self.ckpt_path = Corrector.DEFAULT_CHECKPOINT_PATH[self._default_name]\n",
        "\n",
        "        self.vocab_path = vocab_path or os.path.join(self.ckpt_path, \"vocab.pkl\")\n",
        "        if not os.path.isfile(self.vocab_path):  # leads to \"FileNotFoundError\"\n",
        "            download_pretrained_model(self.ckpt_path)\n",
        "\n",
        "        self.load_output_vocab(self.vocab_path)\n",
        "        self.load_model(self.ckpt_path)\n",
        "\n",
        "        return\n",
        "\n",
        "    def from_pretrained(self, ckpt_path=None, vocab_path=None, **kwargs):\n",
        "        self._from_pretrained(ckpt_path=None, vocab_path=None, **kwargs)\n",
        "\n",
        "    def load_output_vocab(self, vocab_path):\n",
        "        print(f\"loading vocab from path:{vocab_path}\")\n",
        "        self.vocab = load_vocab_dict(vocab_path)\n",
        "\n",
        "    def evaluate(self, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def finetune(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def add_(self, contextual_model, at=\"input\"):\n",
        "        raise Exception(\"this functionality is only available with `SclstmChecker`\")\n",
        "\n",
        "    @abstractmethod\n",
        "    def load_model(self, ckpt_path):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def correct_strings(self, mystrings: List[str], return_all=False):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def get_num_params(self):\n",
        "        self.is_model_ready()\n",
        "        return get_model_nparams(self.model)\n",
        "\n",
        "    def model_size(self, model=None):\n",
        "        if not model:\n",
        "            model = self.model\n",
        "            self.is_model_ready()\n",
        "        sz = {\n",
        "            \"num_params\": get_model_nparams(model),\n",
        "            \"disk_size (in MB)\": get_size_of_model(model),\n",
        "        }\n",
        "        return sz\n",
        "\n",
        "    # new!!\n",
        "    def quantize_model(self, print_stats=False):\n",
        "        self.is_model_ready()\n",
        "\n",
        "        try:\n",
        "            quantized_model = torch.quantization.quantize_dynamic(\n",
        "                self.model, {torch.nn.Linear}, dtype=torch.qint8\n",
        "            )\n",
        "        except RuntimeError as e:\n",
        "            msg = \"Consider moving models to `cpu` by calling `.set_device(device='cpu')` before quantization. \"\n",
        "            raise Exception(msg) from e\n",
        "\n",
        "        if print_stats:\n",
        "            print(\"Before quantization:\")\n",
        "            print(self.model_size())\n",
        "            print(\"After quantization:\")\n",
        "            print(self.model_size(quantized_model))\n",
        "\n",
        "        self.model = quantized_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwJ1Llk6Kwqm"
      },
      "source": [
        "### 4.2.1 SC-LSTM Corrector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXPxV1Dk_fkE"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from neuspell.commons import spacy_tokenizer\n",
        "from neuspell.corrector import Corrector\n",
        "from neuspell.seq_modeling.helpers import load_data\n",
        "from neuspell.seq_modeling.sclstm import load_model, load_pretrained, model_predictions, model_inference\n",
        "from neuspell.util import is_module_available\n",
        "\n",
        "\"\"\" corrector module \"\"\"\n",
        "\n",
        "class SclstmChecker(Corrector):\n",
        "\n",
        "    def load_model(self, ckpt_path):\n",
        "        print(f\"initializing model\")\n",
        "        initialized_model = load_model(self.vocab)\n",
        "        self.model = load_pretrained(initialized_model, self.ckpt_path, device=self.device)\n",
        "\n",
        "    def correct_strings(self, mystrings: List[str], return_all=False) -> List[str]:\n",
        "        self.is_model_ready()\n",
        "        if self.tokenize:\n",
        "            mystrings = [spacy_tokenizer(my_str) for my_str in mystrings]\n",
        "        data = [(line, line) for line in mystrings]\n",
        "        batch_size = 4 if self.device == \"cpu\" else 16\n",
        "        return_strings = model_predictions(self.model, data, self.vocab, device=self.device, batch_size=batch_size)\n",
        "        if return_all:\n",
        "            return mystrings, return_strings\n",
        "        else:\n",
        "            return return_strings\n",
        "\n",
        "    def evaluate(self, clean_file, corrupt_file, data_dir=\"\"):\n",
        "        self.is_model_ready()\n",
        "        data_dir = DEFAULT_TRAINTEST_DATA_PATH if data_dir == \"default\" else data_dir\n",
        "\n",
        "        batch_size = 4 if self.device == \"cpu\" else 16\n",
        "        for x, y, z in zip([data_dir], [clean_file], [corrupt_file]):\n",
        "            print(x, y, z)\n",
        "            test_data = load_data(x, y, z)\n",
        "            _ = model_inference(self.model,\n",
        "                                test_data,\n",
        "                                topk=1,\n",
        "                                device=self.device,\n",
        "                                batch_size=batch_size,\n",
        "                                vocab_=self.vocab)\n",
        "        return\n",
        "\n",
        "    def add_(self, contextual_model, at=\"input\"):\n",
        "        \"\"\"\n",
        "        :param contextual_model: choose one of \"elmo\" or \"bert\"\n",
        "        :param at: choose one of \"input\" or \"output\"\n",
        "        :return: a new checker model with contextual model added\n",
        "        \"\"\"\n",
        "        assert contextual_model in [\"elmo\", \"bert\"]\n",
        "        assert at in [\"input\", \"output\"]\n",
        "\n",
        "        if contextual_model == \"elmo\" and not is_module_available(\"allennlp\"):\n",
        "            raise ImportError(\n",
        "                \"install `allennlp` by running `pip install -r extras-requirements.txt`. See `README.md` for more info.\")\n",
        "\n",
        "        new_checker_name = None\n",
        "        if contextual_model == \"elmo\":\n",
        "            new_checker_name = ElmosclstmChecker if at == \"input\" else SclstmelmoChecker\n",
        "        elif contextual_model == \"bert\":\n",
        "            new_checker_name = BertsclstmChecker if at == \"input\" else SclstmbertChecker\n",
        "\n",
        "        new_checker = new_checker_name(tokenize=self.tokenize,\n",
        "                                       pretrained=True,\n",
        "                                       device=self.device)\n",
        "        print(f\"new model loaded: {new_checker_name}\")\n",
        "        return new_checker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NXpNwgA99dJ"
      },
      "source": [
        "### 4.2.2 Nested-LSTM Corrector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5sdX23zBO9T"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from neuspell.commons import spacy_tokenizer\n",
        "from neuspell.corrector import Corrector\n",
        "from neuspell.seq_modeling.helpers import load_data\n",
        "from neuspell.seq_modeling.lstmlstm import load_model, load_pretrained, model_predictions, model_inference\n",
        "\n",
        "\"\"\" corrector module \"\"\"\n",
        "\n",
        "\n",
        "class NestedlstmChecker(Corrector):\n",
        "\n",
        "    def load_model(self, ckpt_path):\n",
        "        print(f\"initializing model\")\n",
        "        initialized_model = load_model(self.vocab)\n",
        "        self.model = load_pretrained(initialized_model, self.ckpt_path, device=self.device)\n",
        "\n",
        "    def correct_strings(self, mystrings: List[str], return_all=False) -> List[str]:\n",
        "        self.is_model_ready()\n",
        "        if self.tokenize:\n",
        "            mystrings = [spacy_tokenizer(my_str) for my_str in mystrings]\n",
        "        data = [(line, line) for line in mystrings]\n",
        "        batch_size = 4 if self.device == \"cpu\" else 16\n",
        "        return_strings = model_predictions(self.model, data, self.vocab, device=self.device, batch_size=batch_size)\n",
        "        if return_all:\n",
        "            return mystrings, return_strings\n",
        "        else:\n",
        "            return return_strings\n",
        "\n",
        "    def evaluate(self, clean_file, corrupt_file, data_dir=\"\"):\n",
        "        self.is_model_ready()\n",
        "        data_dir = DEFAULT_TRAINTEST_DATA_PATH if data_dir == \"default\" else data_dir\n",
        "\n",
        "        batch_size = 4 if self.device == \"cpu\" else 16\n",
        "        for x, y, z in zip([data_dir], [clean_file], [corrupt_file]):\n",
        "            print(x, y, z)\n",
        "            test_data = load_data(x, y, z)\n",
        "            _ = model_inference(self.model,\n",
        "                                test_data,\n",
        "                                topk=1,\n",
        "                                device=self.device,\n",
        "                                batch_size=batch_size,\n",
        "                                vocab_=self.vocab)\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK7yarow92dD"
      },
      "source": [
        "### 4.2.3 CNN-LSTM Corrector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD0wskaC3Uc3"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from neuspell.commons import spacy_tokenizer\n",
        "from neuspell.corrector import Corrector\n",
        "from neuspell.seq_modeling.cnnlstm import load_model, load_pretrained, model_predictions, model_inference\n",
        "from neuspell.seq_modeling.helpers import load_data\n",
        "\n",
        "\"\"\" corrector module \"\"\"\n",
        "\n",
        "\n",
        "class CnnlstmChecker(Corrector):\n",
        "\n",
        "    def load_model(self, ckpt_path):\n",
        "        print(f\"initializing model\")\n",
        "        initialized_model = load_model(self.vocab)\n",
        "        self.model = load_pretrained(initialized_model, self.ckpt_path, device=self.device)\n",
        "\n",
        "    def correct_strings(self, mystrings: List[str], return_all=False) -> List[str]:\n",
        "        self.is_model_ready()\n",
        "        if self.tokenize:\n",
        "            mystrings = [spacy_tokenizer(my_str) for my_str in mystrings]\n",
        "        data = [(line, line) for line in mystrings]\n",
        "        batch_size = 4 if self.device == \"cpu\" else 16\n",
        "        return_strings = model_predictions(self.model, data, self.vocab, device=self.device, batch_size=batch_size)\n",
        "        if return_all:\n",
        "            return mystrings, return_strings\n",
        "        else:\n",
        "            return return_strings\n",
        "\n",
        "    def evaluate(self, clean_file, corrupt_file, data_dir=\"\"):\n",
        "        self.is_model_ready()\n",
        "        data_dir = DEFAULT_TRAINTEST_DATA_PATH if data_dir == \"default\" else data_dir\n",
        "\n",
        "        batch_size = 4 if self.device == \"cpu\" else 16\n",
        "        for x, y, z in zip([data_dir], [clean_file], [corrupt_file]):\n",
        "            print(x, y, z)\n",
        "            test_data = load_data(x, y, z)\n",
        "            _ = model_inference(self.model,\n",
        "                                test_data,\n",
        "                                topk=1,\n",
        "                                device=self.device,\n",
        "                                batch_size=batch_size,\n",
        "                                vocab_=self.vocab)\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGiyfzAZmrLx"
      },
      "source": [
        "### 4.2.4 BERT Corrector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGjXHl47h-Y1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from typing import List, Dict, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "\n",
        "from neuspell.commons import DEFAULT_TRAINTEST_DATA_PATH\n",
        "from neuspell.corrector import Corrector\n",
        "from neuspell.seq_modeling.helpers import bert_tokenize_for_valid_examples\n",
        "from neuspell.seq_modeling.helpers import load_data, load_vocab_dict, save_vocab_dict\n",
        "from neuspell.seq_modeling.helpers import train_validation_split, batch_iter, labelize, progressBar, batch_accuracy_func\n",
        "from neuspell.seq_modeling.subwordbert import load_model, load_pretrained, model_predictions, model_inference\n",
        "\n",
        "\"\"\" corrector module \"\"\"\n",
        "\n",
        "\n",
        "class BertChecker(Corrector):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.bert_pretrained_name_or_path = \"bert-base-cased\"\n",
        "\n",
        "    def load_model(self, ckpt_path):\n",
        "        print(f\"initializing model\")\n",
        "        initialized_model = load_model(self.vocab)\n",
        "        self.model = load_pretrained(initialized_model, self.ckpt_path, device=self.device)\n",
        "\n",
        "    def correct_strings(self, mystrings: List[str], return_all=False) -> List[str]:\n",
        "        self.is_model_ready()\n",
        "        mystrings = bert_tokenize_for_valid_examples(mystrings, mystrings, self.bert_pretrained_name_or_path)[0]\n",
        "        data = [(line, line) for line in mystrings]\n",
        "        batch_size = 4 if self.device == \"cpu\" else 16\n",
        "        return_strings = model_predictions(self.model, data, self.vocab, device=self.device, batch_size=batch_size)\n",
        "        if return_all:\n",
        "            return mystrings, return_strings\n",
        "        else:\n",
        "            return return_strings\n",
        "\n",
        "    def evaluate(self, clean_file, corrupt_file, data_dir=\"\"):\n",
        "        self.is_model_ready()\n",
        "        data_dir = DEFAULT_TRAINTEST_DATA_PATH if data_dir == \"default\" else data_dir\n",
        "\n",
        "        batch_size = 4 if self.device == \"cpu\" else 16\n",
        "        for x, y, z in zip([data_dir], [clean_file], [corrupt_file]):\n",
        "            print(x, y, z)\n",
        "            test_data = load_data(x, y, z)\n",
        "            _ = model_inference(self.model,\n",
        "                                test_data,\n",
        "                                topk=1,\n",
        "                                device=self.device,\n",
        "                                batch_size=batch_size,\n",
        "                                vocab_=self.vocab)\n",
        "        return\n",
        "\n",
        "    def from_huggingface(self, bert_pretrained_name_or_path, vocab: Union[Dict, str]):\n",
        "        self.bert_pretrained_name_or_path = bert_pretrained_name_or_path\n",
        "        if isinstance(vocab, str) and os.path.exists(vocab):\n",
        "            self.vocab_path = vocab\n",
        "            print(f\"loading vocab from path:{self.vocab_path}\")\n",
        "            self.vocab = load_vocab_dict(self.vocab_path)\n",
        "        elif isinstance(vocab, dict):\n",
        "            self.vocab = vocab\n",
        "        else:\n",
        "            raise ValueError(f\"unknown vocab type or unable to find path: {type(vocab)}\")\n",
        "        self.model = load_model(self.vocab, bert_pretrained_name_or_path=self.bert_pretrained_name_or_path)\n",
        "        self.model.to(self.device)\n",
        "        return\n",
        "\n",
        "    def finetune(self,\n",
        "                 clean_file,\n",
        "                 corrupt_file,\n",
        "                 data_dir=\"\",\n",
        "                 validation_split=0.2,\n",
        "                 n_epochs=2,\n",
        "                 new_vocab_list: List = None):\n",
        "\n",
        "        if new_vocab_list:\n",
        "            raise NotImplementedError(\"Do not currently support modifying output vocabulary of the models \"\n",
        "                                      \"in the finetune step; however, new vocab is accepted at training time.\")\n",
        "\n",
        "        # load data and split in train-validation\n",
        "        data_dir = DEFAULT_TRAINTEST_DATA_PATH if data_dir == \"default\" else data_dir\n",
        "        train_data = load_data(data_dir, clean_file, corrupt_file)\n",
        "        train_data, valid_data = train_validation_split(train_data, 0.8, seed=11690)\n",
        "        print(\"len of train and test data: \", len(train_data), len(valid_data))\n",
        "\n",
        "        # load vocab and model\n",
        "        self.is_model_ready()\n",
        "\n",
        "        # finetune\n",
        "        #############################################\n",
        "        # training and validation\n",
        "        #############################################\n",
        "        model, vocab = self.model, self.vocab\n",
        "        TRAIN_BATCH_SIZE, VALID_BATCH_SIZE = 16, 32\n",
        "        GRADIENT_ACC = 4\n",
        "        DEVICE = self.device\n",
        "        START_EPOCH, N_EPOCHS = 0, n_epochs\n",
        "        CHECKPOINT_PATH = os.path.join(self.ckpt_path if self.ckpt_path else data_dir, \"new_models\",\n",
        "                                       os.path.split(self.bert_pretrained_name_or_path)[-1])\n",
        "        if os.path.exists(CHECKPOINT_PATH):\n",
        "            num = 1\n",
        "            while True:\n",
        "                NEW_CHECKPOINT_PATH = CHECKPOINT_PATH + f\"-{num}\"\n",
        "                if not os.path.exists(NEW_CHECKPOINT_PATH):\n",
        "                    break\n",
        "                num += 1\n",
        "            CHECKPOINT_PATH = NEW_CHECKPOINT_PATH\n",
        "        VOCAB_PATH = os.path.join(CHECKPOINT_PATH, \"vocab.pkl\")\n",
        "        if not os.path.exists(CHECKPOINT_PATH):\n",
        "            os.makedirs(CHECKPOINT_PATH)\n",
        "        print(f\"CHECKPOINT_PATH: {CHECKPOINT_PATH}\")\n",
        "\n",
        "        # running stats\n",
        "        max_dev_acc, argmax_dev_acc = -1, -1\n",
        "        patience = 100\n",
        "\n",
        "        # Create an optimizer\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "        t_total = int(len(train_data) / TRAIN_BATCH_SIZE / GRADIENT_ACC * N_EPOCHS)\n",
        "        if t_total == 0:\n",
        "            t_total = 1\n",
        "        optimizer = BertAdam(optimizer_grouped_parameters, lr=5e-5, warmup=0.1, t_total=t_total)\n",
        "\n",
        "        # model to device\n",
        "        model.to(DEVICE)\n",
        "\n",
        "        # load parameters if not training from scratch\n",
        "        if START_EPOCH > 1:\n",
        "            progress_write_file = (\n",
        "                open(os.path.join(CHECKPOINT_PATH, f\"progress_retrain_from_epoch{START_EPOCH}.txt\"), 'w')\n",
        "            )\n",
        "            model, optimizer, max_dev_acc, argmax_dev_acc = load_pretrained(model, CHECKPOINT_PATH, optimizer=optimizer)\n",
        "            progress_write_file.write(f\"Training model params after loading from path: {CHECKPOINT_PATH}\\n\")\n",
        "        else:\n",
        "            progress_write_file = open(os.path.join(CHECKPOINT_PATH, \"progress.txt\"), 'w')\n",
        "            print(f\"Training model params\")\n",
        "            progress_write_file.write(f\"Training model params\\n\")\n",
        "        progress_write_file.flush()\n",
        "\n",
        "        # train and eval\n",
        "        for epoch_id in range(START_EPOCH, N_EPOCHS + 1):\n",
        "            # check for patience\n",
        "            if (epoch_id - argmax_dev_acc) > patience:\n",
        "                print(\"patience count reached. early stopping initiated\")\n",
        "                print(\"max_dev_acc: {}, argmax_dev_acc: {}\".format(max_dev_acc, argmax_dev_acc))\n",
        "                break\n",
        "            # print epoch\n",
        "            print(f\"In epoch: {epoch_id}\")\n",
        "            progress_write_file.write(f\"In epoch: {epoch_id}\\n\")\n",
        "            progress_write_file.flush()\n",
        "            # train loss and backprop\n",
        "            train_loss = 0.\n",
        "            train_acc = 0.\n",
        "            train_acc_count = 0.\n",
        "            print(\"train_data size: {}\".format(len(train_data)))\n",
        "            progress_write_file.write(\"train_data size: {}\\n\".format(len(train_data)))\n",
        "            progress_write_file.flush()\n",
        "            train_data_iter = batch_iter(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "            nbatches = int(np.ceil(len(train_data) / TRAIN_BATCH_SIZE))\n",
        "            optimizer.zero_grad()\n",
        "            for batch_id, (batch_labels, batch_sentences) in enumerate(train_data_iter):\n",
        "                st_time = time.time()\n",
        "                # set batch data for bert\n",
        "                batch_labels_, batch_sentences_, batch_bert_inp, batch_bert_splits = \\\n",
        "                    bert_tokenize_for_valid_examples(batch_labels, batch_sentences, self.bert_pretrained_name_or_path)\n",
        "                if len(batch_labels_) == 0:\n",
        "                    print(\"################\")\n",
        "                    print(\"Not training the following lines due to pre-processing mismatch: \\n\")\n",
        "                    print([(a, b) for a, b in zip(batch_labels, batch_sentences)])\n",
        "                    print(\"################\")\n",
        "                    continue\n",
        "                else:\n",
        "                    batch_labels, batch_sentences = batch_labels_, batch_sentences_\n",
        "                batch_bert_inp = {k: v.to(DEVICE) for k, v in batch_bert_inp.items()}\n",
        "                # set batch data for others\n",
        "                batch_labels, batch_lengths = labelize(batch_labels, vocab)\n",
        "                # batch_lengths = batch_lengths.to(device)\n",
        "                batch_labels = batch_labels.to(DEVICE)\n",
        "                # forward\n",
        "                model.train()\n",
        "                loss = model(batch_bert_inp, batch_bert_splits, targets=batch_labels)\n",
        "                batch_loss = loss.cpu().detach().numpy()\n",
        "                train_loss += batch_loss\n",
        "                # backward\n",
        "                if GRADIENT_ACC > 1:\n",
        "                    loss = loss / GRADIENT_ACC\n",
        "                loss.backward()\n",
        "                # step\n",
        "                if (batch_id + 1) % GRADIENT_ACC == 0 or batch_id >= nbatches - 1:\n",
        "                    # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "                    # scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "                # compute accuracy in numpy\n",
        "                if batch_id % 10000 == 0:\n",
        "                    train_acc_count += 1\n",
        "                    model.eval()\n",
        "                    with torch.no_grad():\n",
        "                        _, batch_predictions = model(batch_bert_inp, batch_bert_splits, targets=batch_labels)\n",
        "                    model.train()\n",
        "                    batch_labels = batch_labels.cpu().detach().numpy()\n",
        "                    batch_lengths = batch_lengths.cpu().detach().numpy()\n",
        "                    ncorr, ntotal = batch_accuracy_func(batch_predictions, batch_labels, batch_lengths)\n",
        "                    batch_acc = ncorr / ntotal\n",
        "                    train_acc += batch_acc\n",
        "                    # update progress\n",
        "                progressBar(batch_id + 1,\n",
        "                            int(np.ceil(len(train_data) / TRAIN_BATCH_SIZE)),\n",
        "                            [\"batch_time\", \"batch_loss\", \"avg_batch_loss\", \"batch_acc\", \"avg_batch_acc\"],\n",
        "                            [time.time() - st_time, batch_loss, train_loss / (batch_id + 1), batch_acc,\n",
        "                             train_acc / train_acc_count])\n",
        "                if batch_id == 0 or (batch_id + 1) % 5000 == 0:\n",
        "                    nb = int(np.ceil(len(train_data) / TRAIN_BATCH_SIZE))\n",
        "                    progress_write_file.write(f\"{batch_id + 1}/{nb}\\n\")\n",
        "                    progress_write_file.write(\n",
        "                        f\"batch_time: {time.time() - st_time}, avg_batch_loss: {train_loss / (batch_id + 1)}, \"\n",
        "                        f\"avg_batch_acc: {train_acc / train_acc_count}\\n\")\n",
        "                    progress_write_file.flush()\n",
        "            print(f\"\\nEpoch {epoch_id} train_loss: {train_loss / (batch_id + 1)}\")\n",
        "\n",
        "            # valid loss\n",
        "            valid_loss = 0.\n",
        "            valid_acc = 0.\n",
        "            print(\"valid_data size: {}\".format(len(valid_data)))\n",
        "            progress_write_file.write(\"valid_data size: {}\\n\".format(len(valid_data)))\n",
        "            progress_write_file.flush()\n",
        "            valid_data_iter = batch_iter(valid_data, batch_size=VALID_BATCH_SIZE, shuffle=False)\n",
        "            for batch_id, (batch_labels, batch_sentences) in enumerate(valid_data_iter):\n",
        "                st_time = time.time()\n",
        "                # set batch data for bert\n",
        "                batch_labels_, batch_sentences_, batch_bert_inp, batch_bert_splits = \\\n",
        "                    bert_tokenize_for_valid_examples(batch_labels, batch_sentences, self.bert_pretrained_name_or_path)\n",
        "                if len(batch_labels_) == 0:\n",
        "                    print(\"################\")\n",
        "                    print(\"Not validating the following lines due to pre-processing mismatch: \\n\")\n",
        "                    print([(a, b) for a, b in zip(batch_labels, batch_sentences)])\n",
        "                    print(\"################\")\n",
        "                    continue\n",
        "                else:\n",
        "                    batch_labels, batch_sentences = batch_labels_, batch_sentences_\n",
        "                batch_bert_inp = {k: v.to(DEVICE) for k, v in batch_bert_inp.items()}\n",
        "                # set batch data for others\n",
        "                batch_labels, batch_lengths = labelize(batch_labels, vocab)\n",
        "                # batch_lengths = batch_lengths.to(device)\n",
        "                batch_labels = batch_labels.to(DEVICE)\n",
        "                # forward\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    batch_loss, batch_predictions = model(batch_bert_inp, batch_bert_splits, targets=batch_labels)\n",
        "                model.train()\n",
        "                valid_loss += batch_loss\n",
        "                # compute accuracy in numpy\n",
        "                batch_labels = batch_labels.cpu().detach().numpy()\n",
        "                batch_lengths = batch_lengths.cpu().detach().numpy()\n",
        "                ncorr, ntotal = batch_accuracy_func(batch_predictions, batch_labels, batch_lengths)\n",
        "                batch_acc = ncorr / ntotal\n",
        "                valid_acc += batch_acc\n",
        "                # update progress\n",
        "                progressBar(batch_id + 1,\n",
        "                            int(np.ceil(len(valid_data) / VALID_BATCH_SIZE)),\n",
        "                            [\"batch_time\", \"batch_loss\", \"avg_batch_loss\", \"batch_acc\", \"avg_batch_acc\"],\n",
        "                            [time.time() - st_time, batch_loss, valid_loss / (batch_id + 1), batch_acc,\n",
        "                             valid_acc / (batch_id + 1)])\n",
        "                if batch_id == 0 or (batch_id + 1) % 2000 == 0:\n",
        "                    nb = int(np.ceil(len(valid_data) / VALID_BATCH_SIZE))\n",
        "                    progress_write_file.write(f\"{batch_id}/{nb}\\n\")\n",
        "                    progress_write_file.write(\n",
        "                        f\"batch_time: {time.time() - st_time}, avg_batch_loss: {valid_loss / (batch_id + 1)}, \"\n",
        "                        f\"avg_batch_acc: {valid_acc / (batch_id + 1)}\\n\")\n",
        "                    progress_write_file.flush()\n",
        "            print(f\"\\nEpoch {epoch_id} valid_loss: {valid_loss / (batch_id + 1)}\")\n",
        "\n",
        "            # save model, optimizer and test_predictions if val_acc is improved\n",
        "            if valid_acc >= max_dev_acc:\n",
        "                print(f\"validation accuracy improved from {max_dev_acc:.4f} to {valid_acc:.4f}\")\n",
        "                # name = \"model.pth.tar\".format(epoch_id)\n",
        "                # torch.save({\n",
        "                #     'epoch_id': epoch_id,\n",
        "                #     'max_dev_acc': max_dev_acc,\n",
        "                #     'argmax_dev_acc': argmax_dev_acc,\n",
        "                #     'model_state_dict': model.state_dict(),\n",
        "                #     'optimizer_state_dict': optimizer.state_dict()},\n",
        "                #     os.path.join(CHECKPOINT_PATH, name))\n",
        "                name = \"pytorch_model.bin\"\n",
        "                torch.save(model.state_dict(), os.path.join(CHECKPOINT_PATH, name))\n",
        "                print(\"Model saved at {} in epoch {}\".format(os.path.join(CHECKPOINT_PATH, name), epoch_id))\n",
        "                save_vocab_dict(VOCAB_PATH, vocab)\n",
        "\n",
        "                # re-assign\n",
        "                max_dev_acc, argmax_dev_acc = valid_acc, epoch_id\n",
        "\n",
        "        print(f\"Model and logs saved at {CHECKPOINT_PATH}\")\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV2WP2F6Rz07"
      },
      "source": [
        "# 5. Commmand line Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itQY4EMwEVH_"
      },
      "source": [
        "## Loading each Neural Model and Testing for context aware spelling correction via a unified command line\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GKZrWpkTNeR"
      },
      "outputs": [],
      "source": [
        "import neuspell\n",
        "\n",
        "from neuspell import SclstmChecker, NestedlstmChecker, CnnlstmChecker, BertChecker\n",
        "TRAIN_TEST_DATA_PATH = neuspell.commons.DEFAULT_TRAINTEST_DATA_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AirwaiTQKcT",
        "outputId": "e7bb3779-2bd2-430c-b1ab-656c3d7ac11b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-11-20 23:39:11.522206: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-20 23:39:11.522272: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-20 23:39:11.522318: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-20 23:39:12.659024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.3.0) (3.3.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.11)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (23.2)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.5.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2023.7.22)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.6.0\n",
            "    Uninstalling en-core-web-sm-3.6.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.6.0\n",
            "Successfully installed en-core-web-sm-3.3.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSYIuTX3Hu78"
      },
      "source": [
        "## 5.1 SC-LSTM Checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO-yzyjJq_8G",
        "outputId": "81bde8e0-d656-4720-f89d-121caf5d73e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/scrnn-probwordnoise/vocab.pkl\n",
            "initializing model\n",
            "Number of parameters in the model: 112111266\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/scrnn-probwordnoise\n"
          ]
        }
      ],
      "source": [
        "checker_sclstm = SclstmChecker\n",
        "checker_sclstm.from_pretrained()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJC3KfHwj_rH"
      },
      "source": [
        "### Test for rectification of misspellings in individual sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "ZRNOgbZdq_v0",
        "outputId": "d0b516ed-c072-4594-d2bf-257229e2d830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "creating spacy models ...\n",
            "spacy models initialized\n",
            "###############################################\n",
            "data size: 1\n",
            "total inference time for this data is: 0.192520 secs\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This looks like a good project'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checker_sclstm.correct(\"This luks like a gud project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGY42iRjkNJV"
      },
      "source": [
        "### Test for rectification of misspellings in sentences with homophones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is8lad26L10e",
        "outputId": "a62f727a-1990-46e6-d075-93c2f97e9b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###############################################\n",
            "data size: 1\n",
            "total inference time for this data is: 0.015982 secs\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['I feel weak this whole week .. I think I might get fever']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checker_sclstm.correct_strings([\"I feel week this hole week. I think i mite get fiver\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDcQBlMwkOgr"
      },
      "source": [
        "### Test for rectification of misspellings in a list sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JI3Y59nrfGj",
        "outputId": "ec4ae987-e980-4c20-998b-47fe2d60cad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###############################################\n",
            "data size: 1\n",
            "total inference time for this data is: 0.031265 secs\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['These words are often used together . You can go to the definition of spelling or the definition of mistake . Or , see other combinations with mistake .']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checker_sclstm.correct_strings([\"Thee wors are often used together. You can go to the defition of spellig or the defintion of mistae. Or, see other combintions with mistke.\", ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVMW7Y4EkfpV"
      },
      "source": [
        "### Test for rectification of misspellings in a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6_ebtF9VFC6",
        "outputId": "62c4be55-251d-4c8f-d317-19b862af1d9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###############################################\n",
            "data size: 41\n",
            "total inference time for this data is: 0.307114 secs\n",
            "saving results at: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/sample_prediction.txt\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/sample_clean.txt /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/sample_corrupt.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "41it [00:00, 129884.04it/s]\n",
            "41it [00:00, 316696.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3it [00:00, 14.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.13951427986224493\n",
            "total inference time for this data is: 0.218228 secs\n",
            "###############################################\n",
            "total token count: 811\n",
            "_corr2corr:757, _corr2incorr:3, _incorr2corr:38, _incorr2incorr:13\n",
            "accuracy is 0.9802712700369913\n",
            "word correction rate is 0.7450980392156863\n",
            "###############################################\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "checker_sclstm.correct_from_file(src=f\"{TRAIN_TEST_DATA_PATH}/sample_corrupt.txt\",\n",
        "                              dest=f\"{TRAIN_TEST_DATA_PATH}/sample_prediction.txt\")\n",
        "checker_sclstm.evaluate(clean_file=f\"{TRAIN_TEST_DATA_PATH}/sample_clean.txt\", corrupt_file=f\"{TRAIN_TEST_DATA_PATH}/sample_corrupt.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjGe71v9kjW-"
      },
      "source": [
        "### Evaluation of SC-LSTM Checker with BEA-322 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChasEINzm4vC",
        "outputId": "aebf6953-4883-449f-b075-d148ee4f54ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.bea322 /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.bea322.noise\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "322it [00:00, 231459.45it/s]\n",
            "322it [00:00, 992333.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "21it [00:01, 12.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.5217444726399013\n",
            "total inference time for this data is: 1.735731 secs\n",
            "###############################################\n",
            "total token count: 5432\n",
            "_corr2corr:4941, _corr2incorr:168, _incorr2corr:213, _incorr2incorr:110\n",
            "accuracy is 0.948821796759941\n",
            "word correction rate is 0.6594427244582043\n",
            "###############################################\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "checker_sclstm.evaluate(clean_file=f\"{TRAIN_TEST_DATA_PATH}/test.bea322\", corrupt_file=f\"{TRAIN_TEST_DATA_PATH}/test.bea322.noise\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjPdcAOvMRXw"
      },
      "source": [
        "## 5.2 Nested-LSTM Checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL3naLo0qtG7",
        "outputId": "e861f706-2f98-498b-e831-343f11e304e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/lstm-lstm-probwordnoise/vocab.pkl\n",
            "initializing model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters in the model: 113748203\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/lstm-lstm-probwordnoise\n"
          ]
        }
      ],
      "source": [
        "checker_nestedlstm = NestedlstmChecker()\n",
        "checker_nestedlstm.from_pretrained()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn3QGRtGl_-e"
      },
      "source": [
        "\n",
        "### Test for rectification of misspellings in individual sentences\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "5D9u1fgeqtrn",
        "outputId": "10f6e448-3073-48e1-f6d6-c136abc122e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###############################################\n",
            "data size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 29.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total inference time for this data is: 0.175193 secs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This looks like a good project'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checker_nestedlstm.correct(\"This luks like a gud project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36Zo4SFDoHO_"
      },
      "source": [
        "### Test for rectification of misspellings in sentences with homophones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seOG8fcUNff0",
        "outputId": "ae7086f9-4d0d-45bf-911b-36a9020f7dd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###############################################\n",
            "data size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 56.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total inference time for this data is: 0.025057 secs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['I feel week this whole week . I think I might get fever']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checker_nestedlstm.correct_strings([\"I feel week this hole week. I think i mite get fiver\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0xAq-R8pV6j"
      },
      "source": [
        "### Test for rectification of misspellings in a list sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5ddDvBBqz_V",
        "outputId": "ac49b2a9-05c8-4c6e-83fe-d99a59e158ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###############################################\n",
            "data size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 30.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total inference time for this data is: 0.040169 secs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['These words are often used together . You can go to the definition of spelling or the definition of mistake . Or , see other combinations with mistake .']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checker_nestedlstm.correct_strings([\"Thee wors are often used together. You can go to the defition of spellig or the defintion of mistae. Or, see other combintions with mistke.\", ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ps6Z98p1rO"
      },
      "source": [
        "### Test for rectification of misspellings in a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKtIsxS1Y4JD",
        "outputId": "6705370a-0169-42ef-b5f6-c83031cf55ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###############################################\n",
            "data size: 41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3it [00:00,  8.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total inference time for this data is: 0.384470 secs\n",
            "saving results at: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/sample_prediction.txt\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/sample_clean.txt /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/sample_corrupt.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "41it [00:00, 143186.06it/s]\n",
            "41it [00:00, 390832.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3it [00:00, 10.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.14379747211933136\n",
            "total inference time for this data is: 0.308594 secs\n",
            "###############################################\n",
            "total token count: 811\n",
            "_corr2corr:756, _corr2incorr:4, _incorr2corr:40, _incorr2incorr:11\n",
            "accuracy is 0.9815043156596794\n",
            "word correction rate is 0.7843137254901961\n",
            "###############################################\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "checker_nestedlstm.correct_from_file(src=f\"{TRAIN_TEST_DATA_PATH}/sample_corrupt.txt\",\n",
        "                              dest=f\"{TRAIN_TEST_DATA_PATH}/sample_prediction.txt\")\n",
        "checker_nestedlstm.evaluate(f\"{TRAIN_TEST_DATA_PATH}/sample_clean.txt\", f\"{TRAIN_TEST_DATA_PATH}/sample_corrupt.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoEoy8_4R-Cy"
      },
      "source": [
        "## 5.3 CNN-LSTM Checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O2lWwxpqdLJ",
        "outputId": "3389b822-4128-428a-ecbf-3cb3cdbcaf4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/cnn-lstm-probwordnoise/vocab.pkl\n",
            "initializing model\n",
            "Number of parameters in the model: 112480792\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/cnn-lstm-probwordnoise\n"
          ]
        }
      ],
      "source": [
        "checker_cnnlstm = CnnlstmChecker()\n",
        "checker_cnnlstm.from_pretrained()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sShFdpptmMB5"
      },
      "source": [
        "### Test for rectification of misspellings in individual sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "ZkOcKYYvqqB7",
        "outputId": "90d7d85e-6c3c-4831-cc44-2e75a64d7a1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###############################################\n",
            "data size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 12.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total inference time for this data is: 0.223005 secs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This looks like a good project'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checker_cnnlstm.correct(\"This luks like a gud project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bInMg6rioJJc"
      },
      "source": [
        "### Test for rectification of misspellings in sentences with homophones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6G36WXiCvG4",
        "outputId": "e5e93d8d-c7c4-4edf-e1f3-2b85b084058e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###############################################\n",
            "data size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 49.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total inference time for this data is: 0.034465 secs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['I feel weak this whole week . I think I might get fiber']"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checker_cnnlstm.correct_strings([\"I feel week this hole week. I think i mite get fiver\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7XdEtZSpYFj"
      },
      "source": [
        "### Test for rectification of misspellings in a list sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaXQWe-lqbVq",
        "outputId": "6116b1fa-919f-47ba-8757-a2c4a6d307b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###############################################\n",
            "data size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 32.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total inference time for this data is: 0.039763 secs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Three words are often used together . You can go to the definition of spelling or the definition of mistake . Or , see other combinations with mistake .']"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checker_cnnlstm.correct_strings([\"Thee wors are often used together. You can go to the defition of spellig or the defintion of mistae. Or, see other combintions with mistke.\", ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OwEIPapp39m"
      },
      "source": [
        "### Test for rectification of misspellings in a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OBv0egocLvt",
        "outputId": "344c18fe-e541-4184-ba47-1d7f759076ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###############################################\n",
            "data size: 41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3it [00:00,  9.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total inference time for this data is: 0.333380 secs\n",
            "saving results at: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/sample_prediction.txt\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/sample_clean.txt /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/sample_corrupt.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "41it [00:00, 92854.46it/s]\n",
            "41it [00:00, 384712.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3it [00:00, 14.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.14645865062872568\n",
            "total inference time for this data is: 0.222200 secs\n",
            "###############################################\n",
            "total token count: 811\n",
            "_corr2corr:754, _corr2incorr:6, _incorr2corr:36, _incorr2incorr:15\n",
            "accuracy is 0.9741060419235512\n",
            "word correction rate is 0.7058823529411765\n",
            "###############################################\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "checker_cnnlstm.correct_from_file(src=f\"{TRAIN_TEST_DATA_PATH}/sample_corrupt.txt\",\n",
        "                              dest=f\"{TRAIN_TEST_DATA_PATH}/sample_prediction.txt\")\n",
        "checker_cnnlstm.evaluate(f\"{TRAIN_TEST_DATA_PATH}/sample_clean.txt\", f\"{TRAIN_TEST_DATA_PATH}/sample_corrupt.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfdVGAcLUVHM"
      },
      "source": [
        "## 5.4 BERT Checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "2f6a9efdbb014e8fa974bebb2d9c2249",
            "a62a74b5db764b1ab8b6f4d456386b2d",
            "cfcb10243eab4af69ec851536e999288",
            "99a2c3adb9554d9f892a16fb5cef0688",
            "8c353a482db14c0aad02599661668bdf",
            "fead061b85314144a53f2d636ccb981b",
            "018a17804b2d4c729b317551546d9ff1",
            "88ddc7a84b694f2fb6364be622ef54a4",
            "2cfb58a8ad9744648de87a556720306f",
            "585f6066ae144797a64ed6a665d4a876",
            "c151e5003ee446cab6b83ca260ff7f4f",
            "13f8135a00244b589d18424d6c01efce",
            "0672f50de9fd4275a9519318d1f50f36",
            "176998ebcc134ba0a9d427b9edb7aad0",
            "ea12bb6b930244b9a86a1beebac3aee3",
            "e91e4d45e86341e3b2c8d5b8cc6fe9d7",
            "65ad0148a17f4ed4a260fe92c07aa79b",
            "72d7d98dd5d643bead124d02cad32fc1",
            "af1043ab0a0a4db7a0b75b10aa26a2d8",
            "3730486c1d1a4b848b7551d098b48d3b",
            "6da37c59ba50438a97ade5438fec01c9",
            "d018ce7778b74fbbbf19ca23b392f792"
          ]
        },
        "id": "xSjExQhr2oiu",
        "outputId": "cb23ae1f-f9e8-4f67-f1fb-ac83e7a6c118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/subwordbert-probwordnoise/vocab.pkl\n",
            "initializing model\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f6a9efdbb014e8fa974bebb2d9c2249",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13f8135a00244b589d18424d6c01efce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters in the model: 185211810\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/subwordbert-probwordnoise\n"
          ]
        }
      ],
      "source": [
        "checker_bert = BertChecker()\n",
        "checker_bert.from_pretrained()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUXw9exkmN4Z"
      },
      "source": [
        "### Test for rectification of misspellings in individual sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "769f8745902c4e5687107f419d99ab53",
            "74f482d67ab34391a0202b516ce50abf",
            "40427afdae0143cfb154d1fd54e8775d",
            "3a874ca4cc4f4d969964ae053d97af6d",
            "c01b42adf48d4eccb1a12b6d45e8a991",
            "66a8cf97d45a4e68a67a6fa4bc60a1e0",
            "7a1cb318cb5f4b3db8a6e53bc2757a87",
            "a6df1233579d4f46a2ce378f595872d6",
            "7e32c13de76f43e486a412a8740fddc3",
            "ccf7480376624033ad9b5d9231b502c0",
            "9841715ffff243c7a424f37b3e831dd6",
            "95d774cabf2144629e338670d76bf92a",
            "055c0e3fa0884689b3d1d883b64cd54d",
            "01270f380dc5465491a83e3e164618a1",
            "75a33f7f513e465e913cf9ff20b90aa4",
            "82f17606b4da47f494f40c9b61dd6691",
            "445b6c451c2c4a559752b0d92982df96",
            "7a64308827884f6cb5c9a0c7dc8fcb9f",
            "25ba184e65684a118154ccd772d271da",
            "62fde0f465e942c4ac349980514f1575",
            "d65e4f4a975746aab6abb40d4ce9e205",
            "24ad04dff1a84f18b4dc97a1cb6c60d8"
          ]
        },
        "id": "pFvyYOprAG0L",
        "outputId": "5f3abebf-6ec0-41e6-c881-0db16a8471c4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "769f8745902c4e5687107f419d99ab53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95d774cabf2144629e338670d76bf92a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This looks like a good project'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checker_bert.correct(\"This luks like a gud project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0K4KwYBoLpV"
      },
      "source": [
        "### Test for rectification of misspellings in sentences with homophones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv3649RPUon4",
        "outputId": "b10af27b-68ef-4b11-a992-f055d927fe25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I feel weak this whole week . I think I might get fever']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checker_bert.correct_strings([\"I feel week this hole week. I think i mite get fiver\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLKmYQ4-paY4"
      },
      "source": [
        "### Test for rectification of misspellings in a list sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH17aTmGAO83",
        "outputId": "24f2f696-37ec-4646-e19c-7372a15cea1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['These words are often used together . You can go to the definition of spelling or the definition of mistake . Or , see other combinations with mistake .']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checker_bert.correct_strings([\"Thee wors are often used together. You can go to the defition of spellig or the defintion of mistae. Or, see other combintions with mistke.\", ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSBx4th9qDve"
      },
      "source": [
        "### Test for rectification of misspellings in a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llp52NS8chtu",
        "outputId": "2c7400f4-463b-48d8-e060-bd4b514a9e7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saving results at: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/sample_prediction.txt\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/sample_clean.txt /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/sample_corrupt.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "41it [00:00, 16791.96it/s]\n",
            "41it [00:00, 390832.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3it [00:00,  8.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.08198519175251325\n",
            "total inference time for this data is: 0.353283 secs\n",
            "###############################################\n",
            "\n",
            "\n",
            "total token count: 819\n",
            "_corr2corr:763, _corr2incorr:5, _incorr2corr:42, _incorr2incorr:9\n",
            "accuracy is 0.9829059829059829\n",
            "word correction rate is 0.8235294117647058\n",
            "###############################################\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "checker_bert.correct_from_file(src=f\"{TRAIN_TEST_DATA_PATH}/sample_corrupt.txt\",\n",
        "                              dest=f\"{TRAIN_TEST_DATA_PATH}/sample_prediction.txt\")\n",
        "checker_bert.evaluate(f\"{TRAIN_TEST_DATA_PATH}/sample_clean.txt\", f\"{TRAIN_TEST_DATA_PATH}/sample_corrupt.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm1Eysqckkby"
      },
      "source": [
        "# 6. Testing the Neuspell Corrector Modules SclstmChecker, NestedlstmChecker, CnnlstmChecker, BertChecker across three (Synthetic, Natural and Ambiguous) different datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiTcvb3skuxz"
      },
      "source": [
        "## 6.1 Synthetic dataset (WORD-TEST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JJw-Iu7t0uj"
      },
      "source": [
        "A word is swapped with its noised counterpart from a prebuilt lookup table. 109K misspelt- correct word pairs were collected for 17K popular English words from a variety of public sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU0RJeprIr3h",
        "outputId": "3645b6c8-69f4-4c56-8d74-16f5b14b3d1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "######################################################\n",
            "checking CnnlstmChecker\n",
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/cnn-lstm-probwordnoise/vocab.pkl\n",
            "initializing model\n",
            "Number of parameters in the model: 112480792\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/cnn-lstm-probwordnoise\n",
            "###############################################\n",
            "data size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 99.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total inference time for this data is: 0.174700 secs\n",
            "to cheque sum spelling rul \n",
            "\t\t→ to check some spelling rule\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.1blm /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.1blm.noise.word\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "273134it [00:01, 201702.85it/s]\n",
            "273134it [00:00, 931569.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 273134\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17071it [22:35, 12.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.11120179020414805\n",
            "total inference time for this data is: 1355.627001 secs\n",
            "###############################################\n",
            "total token count: 6085104\n",
            "_corr2corr:4830355, _corr2incorr:37113, _incorr2corr:1071741, _incorr2incorr:145895\n",
            "accuracy is 0.9699252469637331\n",
            "word correction rate is 0.8801817620372591\n",
            "###############################################\n",
            "######################################################\n",
            "\n",
            "\n",
            "######################################################\n",
            "checking BertChecker\n",
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/subwordbert-probwordnoise/vocab.pkl\n",
            "initializing model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters in the model: 185211810\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/subwordbert-probwordnoise\n",
            "to cheque sum spelling rul \n",
            "\t\t→ to check some spelling role\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.1blm /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.1blm.noise.word\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "273134it [00:00, 358210.63it/s]\n",
            "273134it [00:00, 1564427.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 273134\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17071it [35:23,  8.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.05801963801150533\n",
            "total inference time for this data is: 2123.741728 secs\n",
            "###############################################\n",
            "\n",
            "\n",
            "total token count: 5540149\n",
            "_corr2corr:4493183, _corr2incorr:15270, _incorr2corr:982918, _incorr2incorr:48778\n",
            "accuracy is 0.9884393000982464\n",
            "word correction rate is 0.9527205688497387\n",
            "###############################################\n",
            "######################################################\n",
            "\n",
            "\n",
            "######################################################\n",
            "checking SclstmChecker\n",
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/scrnn-probwordnoise/vocab.pkl\n",
            "initializing model\n",
            "Number of parameters in the model: 112111266\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/scrnn-probwordnoise\n",
            "###############################################\n",
            "data size: 1\n",
            "total inference time for this data is: 0.129848 secs\n",
            "to cheque sum spelling rul \n",
            "\t\t→ to check some spelling rule\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.1blm /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.1blm.noise.word\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "273134it [00:00, 350113.16it/s]\n",
            "273134it [00:00, 1565291.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 273134\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17071it [21:26, 13.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.08433901913774819\n",
            "total inference time for this data is: 1286.026940 secs\n",
            "###############################################\n",
            "total token count: 6085104\n",
            "_corr2corr:4835197, _corr2incorr:32271, _incorr2corr:1101317, _incorr2incorr:116319\n",
            "accuracy is 0.975581354073817\n",
            "word correction rate is 0.9044714512383011\n",
            "###############################################\n",
            "######################################################\n",
            "\n",
            "\n",
            "######################################################\n",
            "checking NestedlstmChecker\n",
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/lstm-lstm-probwordnoise/vocab.pkl\n",
            "initializing model\n",
            "Number of parameters in the model: 113748203\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/lstm-lstm-probwordnoise\n",
            "###############################################\n",
            "data size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 94.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total inference time for this data is: 0.167049 secs\n",
            "to cheque sum spelling rul \n",
            "\t\t→ to check some spelling rule\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.1blm /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.1blm.noise.word\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "273134it [00:01, 207339.33it/s]\n",
            "273134it [00:00, 1000564.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 273134\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17071it [27:31, 10.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.07905739718273122\n",
            "total inference time for this data is: 1651.616775 secs\n",
            "###############################################\n",
            "total token count: 6085104\n",
            "_corr2corr:4841418, _corr2incorr:26050, _incorr2corr:1109714, _incorr2incorr:107922\n",
            "accuracy is 0.9779836137558208\n",
            "word correction rate is 0.9113676008265196\n",
            "###############################################\n",
            "######################################################\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "USAGE\n",
        "-----\n",
        "for gpu (+- cpu) testing:\n",
        ">>> CUDA_VISIBLE_DEVICES=0 python test_neuspell_correctors.py\n",
        "for cpu-only testing:\n",
        ">>> python test_neuspell_correctors.py\n",
        "-----\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "\n",
        "import neuspell\n",
        "#from neuspell import CnnlstmChecker, BertsclstmChecker, NestedlstmChecker, SclstmbertChecker, BertChecker, SclstmChecker\n",
        "from neuspell import CnnlstmChecker, BertChecker, SclstmChecker, NestedlstmChecker\n",
        "from neuspell.seq_modeling.util import is_module_available\n",
        "\n",
        "\n",
        "all_checkers = [\n",
        "    CnnlstmChecker,\n",
        "    BertChecker,\n",
        "    SclstmChecker,\n",
        "    NestedlstmChecker\n",
        "]\n",
        "\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "TRAIN_TEST_DATA_PATH = neuspell.commons.DEFAULT_TRAINTEST_DATA_PATH\n",
        "\n",
        "\n",
        "for Checker in all_checkers:\n",
        "    print(\"\\n######################################################\")\n",
        "    print(f\"checking {Checker.__name__}\")\n",
        "\n",
        "    \"\"\" load a checker from a checkpoint; defaults to load on cpu device \"\"\"\n",
        "    checker = Checker()\n",
        "    checker.from_pretrained()\n",
        "    print(\"to cheque sum spelling rul\", \"\\n\\t\\t→\", checker.correct(\"to cheque sum spelling rul\"))\n",
        "    checker.evaluate(f\"{TRAIN_TEST_DATA_PATH}/test.1blm\", f\"{TRAIN_TEST_DATA_PATH}/test.1blm.noise.word\")\n",
        "\n",
        "    print(\"######################################################\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rqjhgSXJ2iR"
      },
      "source": [
        "## 6.2 Natural JFLEG dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmgp82E93O8a"
      },
      "source": [
        "The JHU FLuency-Extended GUG Corpus (JFLEG) dataset (Napoles et al., 2017) is a collection of essays written by English learners with different first languages. This dataset contains 2K spelling mistakes (6.1% of all tokens) in 1601 sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRxa21peotCS",
        "outputId": "52c1e67c-4cde-4a2a-c327-1233094bb90c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "######################################################\n",
            "checking CnnlstmChecker\n",
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/cnn-lstm-probwordnoise/vocab.pkl\n",
            "initializing model\n",
            "Number of parameters in the model: 112480792\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/cnn-lstm-probwordnoise\n",
            "###############################################\n",
            "data size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 77.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total inference time for this data is: 0.169136 secs\n",
            "to cheque sum spelling rul \n",
            "\t\t→ to check some spelling rule\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.jfleg /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.jfleg.noise\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1601it [00:00, 193741.51it/s]\n",
            "1601it [00:00, 1262470.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 1601\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "101it [00:09, 10.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.18989075727687024\n",
            "total inference time for this data is: 9.999173 secs\n",
            "###############################################\n",
            "total token count: 33625\n",
            "_corr2corr:31168, _corr2incorr:416, _incorr2corr:1635, _incorr2incorr:406\n",
            "accuracy is 0.9755539033457249\n",
            "word correction rate is 0.801077902988731\n",
            "###############################################\n",
            "######################################################\n",
            "\n",
            "\n",
            "######################################################\n",
            "checking BertChecker\n",
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/subwordbert-probwordnoise/vocab.pkl\n",
            "initializing model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters in the model: 185211810\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/subwordbert-probwordnoise\n",
            "to cheque sum spelling rul \n",
            "\t\t→ to check some spelling role\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.jfleg /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.jfleg.noise\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1601it [00:00, 166975.35it/s]\n",
            "1601it [00:00, 410231.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 1601\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "101it [00:14,  7.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.16809210149884815\n",
            "total inference time for this data is: 14.018964 secs\n",
            "###############################################\n",
            "\n",
            "\n",
            "total token count: 33473\n",
            "_corr2corr:31048, _corr2incorr:415, _incorr2corr:1708, _incorr2incorr:302\n",
            "accuracy is 0.9785797508439638\n",
            "word correction rate is 0.8497512437810946\n",
            "###############################################\n",
            "######################################################\n",
            "\n",
            "\n",
            "######################################################\n",
            "checking SclstmChecker\n",
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/scrnn-probwordnoise/vocab.pkl\n",
            "initializing model\n",
            "Number of parameters in the model: 112111266\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/scrnn-probwordnoise\n",
            "###############################################\n",
            "data size: 1\n",
            "total inference time for this data is: 0.159861 secs\n",
            "to cheque sum spelling rul \n",
            "\t\t→ to check some spelling rule\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.jfleg /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.jfleg.noise\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1601it [00:00, 158132.13it/s]\n",
            "1601it [00:00, 1273001.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 1601\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "101it [00:09, 10.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.1935579574454834\n",
            "total inference time for this data is: 9.611472 secs\n",
            "###############################################\n",
            "total token count: 33625\n",
            "_corr2corr:31247, _corr2incorr:337, _incorr2corr:1672, _incorr2incorr:369\n",
            "accuracy is 0.979003717472119\n",
            "word correction rate is 0.8192062714355708\n",
            "###############################################\n",
            "######################################################\n",
            "\n",
            "\n",
            "######################################################\n",
            "checking NestedlstmChecker\n",
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/lstm-lstm-probwordnoise/vocab.pkl\n",
            "initializing model\n",
            "Number of parameters in the model: 113748203\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/lstm-lstm-probwordnoise\n",
            "###############################################\n",
            "data size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 80.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total inference time for this data is: 0.150553 secs\n",
            "to cheque sum spelling rul \n",
            "\t\t→ to check some spelling rule\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.jfleg /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.jfleg.noise\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1601it [00:00, 248660.64it/s]\n",
            "1601it [00:00, 1915857.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 1601\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "101it [00:11,  8.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.1967287675874068\n",
            "total inference time for this data is: 11.600425 secs\n",
            "###############################################\n",
            "total token count: 33625\n",
            "_corr2corr:31168, _corr2incorr:416, _incorr2corr:1665, _incorr2incorr:376\n",
            "accuracy is 0.9764460966542751\n",
            "word correction rate is 0.8157765801077903\n",
            "###############################################\n",
            "######################################################\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "######################################################\n",
        "######################################################\n",
        "\n",
        "for Checker in all_checkers:\n",
        "    print(\"\\n######################################################\")\n",
        "    print(f\"checking {Checker.__name__}\")\n",
        "\n",
        "    \"\"\" load a checker from a checkpoint; defaults to load on cpu device \"\"\"\n",
        "    checker = Checker()\n",
        "    checker.from_pretrained()\n",
        "    print(\"to cheque sum spelling rul\", \"\\n\\t\\t→\", checker.correct(\"to cheque sum spelling rul\"))\n",
        "    checker.evaluate(f\"{TRAIN_TEST_DATA_PATH}/test.jfleg\", f\"{TRAIN_TEST_DATA_PATH}/test.jfleg.noise\")\n",
        "\n",
        "    print(\"######################################################\\n\")\n",
        "\n",
        "######################################################\n",
        "######################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPNjvLSMKdp3"
      },
      "source": [
        "## 6.3 Ambiguous dataset BEA-322"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTzVvcPyu9Zh"
      },
      "source": [
        " Manually prune down list to 322 sentences, with one ambiguous mistake per sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHUXAEZhKpwF",
        "outputId": "2f7649b2-dc9d-4650-a3f9-1ba3702ee469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "######################################################\n",
            "checking CnnlstmChecker\n",
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/cnn-lstm-probwordnoise/vocab.pkl\n",
            "initializing model\n",
            "Number of parameters in the model: 112480792\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/cnn-lstm-probwordnoise\n",
            "###############################################\n",
            "data size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 80.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total inference time for this data is: 0.178789 secs\n",
            "to cheque sum spelling rul \n",
            "\t\t→ to check some spelling rule\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.bea322 /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.bea322.noise\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "322it [00:00, 137939.52it/s]\n",
            "322it [00:00, 987977.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "21it [00:01, 10.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.5197131761482784\n",
            "total inference time for this data is: 1.948956 secs\n",
            "###############################################\n",
            "total token count: 5432\n",
            "_corr2corr:4745, _corr2incorr:364, _incorr2corr:185, _incorr2incorr:138\n",
            "accuracy is 0.9075846833578792\n",
            "word correction rate is 0.5727554179566563\n",
            "###############################################\n",
            "######################################################\n",
            "\n",
            "\n",
            "######################################################\n",
            "checking BertChecker\n",
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/subwordbert-probwordnoise/vocab.pkl\n",
            "initializing model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters in the model: 185211810\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/subwordbert-probwordnoise\n",
            "to cheque sum spelling rul \n",
            "\t\t→ to check some spelling role\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.bea322 /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.bea322.noise\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "322it [00:00, 316588.35it/s]\n",
            "322it [00:00, 1538229.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "21it [00:02,  8.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.6343091783069429\n",
            "total inference time for this data is: 2.620593 secs\n",
            "###############################################\n",
            "\n",
            "\n",
            "total token count: 5559\n",
            "_corr2corr:4911, _corr2incorr:325, _incorr2corr:232, _incorr2incorr:91\n",
            "accuracy is 0.925166396833963\n",
            "word correction rate is 0.718266253869969\n",
            "###############################################\n",
            "######################################################\n",
            "\n",
            "\n",
            "######################################################\n",
            "checking SclstmChecker\n",
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/scrnn-probwordnoise/vocab.pkl\n",
            "initializing model\n",
            "Number of parameters in the model: 112111266\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/scrnn-probwordnoise\n",
            "###############################################\n",
            "data size: 1\n",
            "total inference time for this data is: 0.151161 secs\n",
            "to cheque sum spelling rul \n",
            "\t\t→ to check some spelling rule\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.bea322 /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.bea322.noise\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "322it [00:00, 59446.54it/s]\n",
            "322it [00:00, 844103.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "21it [00:01, 11.95it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.5217444804452714\n",
            "total inference time for this data is: 1.767683 secs\n",
            "###############################################\n",
            "total token count: 5432\n",
            "_corr2corr:4941, _corr2incorr:168, _incorr2corr:213, _incorr2incorr:110\n",
            "accuracy is 0.948821796759941\n",
            "word correction rate is 0.6594427244582043\n",
            "###############################################\n",
            "######################################################\n",
            "\n",
            "\n",
            "######################################################\n",
            "checking NestedlstmChecker\n",
            "loading vocab from path:/content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/lstm-lstm-probwordnoise/vocab.pkl\n",
            "initializing model\n",
            "Number of parameters in the model: 113748203\n",
            "Loading model params from checkpoint dir: /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/checkpoints/lstm-lstm-probwordnoise\n",
            "###############################################\n",
            "data size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 78.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total inference time for this data is: 0.174065 secs\n",
            "to cheque sum spelling rul \n",
            "\t\t→ to check some spelling rule\n",
            " /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.bea322 /content/drive/MyDrive/Colab Notebooks/ECE570/Varsha/neuspell/neuspell/../data/traintest/test.bea322.noise\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "322it [00:00, 226871.47it/s]\n",
            "322it [00:00, 625551.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded tuples of (corr,incorr) examples from \n",
            "###############################################\n",
            "data size: 322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "21it [00:02,  9.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch None valid_loss: 0.4859040656260082\n",
            "total inference time for this data is: 2.175415 secs\n",
            "###############################################\n",
            "total token count: 5432\n",
            "_corr2corr:4759, _corr2incorr:350, _incorr2corr:204, _incorr2incorr:119\n",
            "accuracy is 0.913659793814433\n",
            "word correction rate is 0.631578947368421\n",
            "###############################################\n",
            "######################################################\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for Checker in all_checkers:\n",
        "    print(\"\\n######################################################\")\n",
        "    print(f\"checking {Checker.__name__}\")\n",
        "\n",
        "    \"\"\" load a checker from a checkpoint; defaults to load on cpu device \"\"\"\n",
        "    checker = Checker()\n",
        "    checker.from_pretrained()\n",
        "    print(\"to cheque sum spelling rul\", \"\\n\\t\\t→\", checker.correct(\"to cheque sum spelling rul\"))\n",
        "    checker.evaluate(clean_file=f\"{TRAIN_TEST_DATA_PATH}/test.bea322\", corrupt_file=f\"{TRAIN_TEST_DATA_PATH}/test.bea322.noise\")\n",
        "\n",
        "    print(\"######################################################\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jBgk2pZo11a"
      },
      "source": [
        "# 7. Performance of Implemented Neuspell models\n",
        "\n",
        " Performance of the implemented four different spelling correction models—CNN-LSTM, BERT, SC-LSTM, and Nested-LSTM was assessed across three diverse datasets: Synthetic(WORD-TEST), Natural(JFLEG), and Ambiguous(BEA-322).\n",
        "\n",
        "\n",
        "| **Model**               | **Dataset**          | **Accuracy** | **Word Correction Rate** | **Time per Sentence (ms)** |\n",
        "|-------------------------|----------------------|--------------|---------------------------|-----------------------------|\n",
        "| CNN-LSTM                | Synthetic(WORD-TEST) | 96.99%       | 88.02%                    | 5.0                         |\n",
        "| BERT                    | Synthetic(WORD-TEST) | 98.84%       | 95.27%                    | 7.8                         |\n",
        "| SC-LSTM                 | Synthetic(WORD-TEST) | 97.56%       | 90.45%                    | 4.7                         |\n",
        "| Nested-LSTM             | Synthetic(WORD-TEST) | 97.80%       | 91.14%                    | 6.1                         |\n",
        "| CNN-LSTM                | Natural (JFLEG)       | 97.56%       | 80.11%                    | 6.2                         |\n",
        "| BERT                    | Natural (JFLEG)       | 97.86%       | 84.98%                    | 13.3                        |\n",
        "| SC-LSTM                 | Natural (JFLEG)       | 97.90%       | 81.92%                    | 6.0                         |\n",
        "| Nested-LSTM             | Natural (JFLEG)        | 97.74%       | 81.58%                    | 6.9                         |\n",
        "| CNN-LSTM                | Ambiguous (BEA-322)   | 90.76%       | 57.28%                    | 6.0                         |\n",
        "| BERT                    | Ambiguous (BEA-322)    | 92.52%       | 71.83%                    | 8.1                         |\n",
        "| SC-LSTM                 | Ambiguous (BEA-322)    | 94.88%       | 65.94%                    | 5.5                         |\n",
        "| Nested-LSTM             | Ambiguous (BEA-322)   | 91.37%       | 63.16%                    | 5.4                         |\n",
        "\n",
        "\n",
        "\n",
        "## Discussion\n",
        "\n",
        "**Synthetic(WORD-TEST) Dataset:**\n",
        "- **Accuracy:**\n",
        "  - BERT (98.84%) outperformed other models.\n",
        "  - SC-LSTM (97.56%) and Nested-LSTM (97.80%) showed competitive accuracy.\n",
        "- **Word Correction Rate:**\n",
        "  - BERT led with a rate of 95.27%.\n",
        "  - Nested-LSTM followed closely with 91.14%.\n",
        "- **Efficiency:**\n",
        "  - SC-LSTM demonstrated efficiency with the lowest time per sentence (4.7 ms).\n",
        "\n",
        "**Natural JFLEG Dataset:**\n",
        "- **Accuracy:**\n",
        "  - BERT maintained the lead (97.86%).\n",
        "  - SC-LSTM and Nested-LSTM showed comparable accuracy.\n",
        "- **Word Correction Rate:**\n",
        "  - BERT excelled with a rate of 84.98%.\n",
        "  - SC-LSTM demonstrated competitive performance (81.92%).\n",
        "- **Efficiency:**\n",
        "  - SC-LSTM remained efficient with a time per sentence of 6.0 ms.\n",
        "\n",
        "**Ambiguous BEA-322 Dataset:**\n",
        "- **Accuracy:**\n",
        "  - SC-LSTM led with 94.88% accuracy.\n",
        "  - BERT and CNN-LSTM showed competitive performance.\n",
        "- **Word Correction Rate:**\n",
        "  - BERT demonstrated the highest rate (71.83%).\n",
        "  - SC-LSTM maintained efficiency with the lowest time per sentence (5.5 ms).\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "- **BERT's Dominance:**\n",
        "  - BERT consistently excelled in accuracy and word correction rate.\n",
        "  \n",
        "- **Efficiency vs. Accuracy:**\n",
        "  - SC-LSTM demonstrated efficiency, making it suitable for applications prioritizing processing speed.\n",
        "\n",
        "- **Context Matters:**\n",
        "  - Model choice should consider specific application requirements, balancing accuracy, efficiency, and contextual nuances."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "005c43efd7744d6d88f5ed3afe6c5264": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01270f380dc5465491a83e3e164618a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25ba184e65684a118154ccd772d271da",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62fde0f465e942c4ac349980514f1575",
            "value": 29
          }
        },
        "018a17804b2d4c729b317551546d9ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "055c0e3fa0884689b3d1d883b64cd54d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_445b6c451c2c4a559752b0d92982df96",
            "placeholder": "​",
            "style": "IPY_MODEL_7a64308827884f6cb5c9a0c7dc8fcb9f",
            "value": "Downloading: 100%"
          }
        },
        "0672f50de9fd4275a9519318d1f50f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65ad0148a17f4ed4a260fe92c07aa79b",
            "placeholder": "​",
            "style": "IPY_MODEL_72d7d98dd5d643bead124d02cad32fc1",
            "value": "Downloading: 100%"
          }
        },
        "13f8135a00244b589d18424d6c01efce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0672f50de9fd4275a9519318d1f50f36",
              "IPY_MODEL_176998ebcc134ba0a9d427b9edb7aad0",
              "IPY_MODEL_ea12bb6b930244b9a86a1beebac3aee3"
            ],
            "layout": "IPY_MODEL_e91e4d45e86341e3b2c8d5b8cc6fe9d7"
          }
        },
        "143d82f3b37c4af2b3daf6bc641a120e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15ce3d4a9e2346b6943dc5a092f5c6b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16daad3c524a4bb1b27d7397d995a168": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc5401ee05314946bb02d23e6022ca60",
            "placeholder": "​",
            "style": "IPY_MODEL_ac0dd98e18394c428682d29a9edc1d2f",
            "value": " 446k/446k [00:00&lt;00:00, 14.4MB/s]"
          }
        },
        "176998ebcc134ba0a9d427b9edb7aad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af1043ab0a0a4db7a0b75b10aa26a2d8",
            "max": 435779157,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3730486c1d1a4b848b7551d098b48d3b",
            "value": 435779157
          }
        },
        "21e3b9acb5104aa09c164ef3f8d86ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c7c9aef5835448fbab71380d5fff448",
              "IPY_MODEL_b871755546f347329a08e25b25bc846d",
              "IPY_MODEL_d9a3dad4c264437d9ba109cdfa7d7be2"
            ],
            "layout": "IPY_MODEL_639c305bf781491f8aa8ffaa2126348c"
          }
        },
        "24ad04dff1a84f18b4dc97a1cb6c60d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25ba184e65684a118154ccd772d271da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b88687aba0e4c69aaff74da9ddffe9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c7c9aef5835448fbab71380d5fff448": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43df29b215924373b55e1a79d378a38c",
            "placeholder": "​",
            "style": "IPY_MODEL_69da365bcb2c4760a2bdb3cbe752afbc",
            "value": "Downloading: 100%"
          }
        },
        "2cfb58a8ad9744648de87a556720306f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f6a9efdbb014e8fa974bebb2d9c2249": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a62a74b5db764b1ab8b6f4d456386b2d",
              "IPY_MODEL_cfcb10243eab4af69ec851536e999288",
              "IPY_MODEL_99a2c3adb9554d9f892a16fb5cef0688"
            ],
            "layout": "IPY_MODEL_8c353a482db14c0aad02599661668bdf"
          }
        },
        "3730486c1d1a4b848b7551d098b48d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "377c24f38a534b95b3f5b030a58479e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38964e8bc5434b00abed61ac76fc0635": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a874ca4cc4f4d969964ae053d97af6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccf7480376624033ad9b5d9231b502c0",
            "placeholder": "​",
            "style": "IPY_MODEL_9841715ffff243c7a424f37b3e831dd6",
            "value": " 208k/208k [00:00&lt;00:00, 7.31MB/s]"
          }
        },
        "40427afdae0143cfb154d1fd54e8775d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6df1233579d4f46a2ce378f595872d6",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e32c13de76f43e486a412a8740fddc3",
            "value": 213450
          }
        },
        "43df29b215924373b55e1a79d378a38c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "445b6c451c2c4a559752b0d92982df96": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "449be78d7b094bdd8abc81290fb26d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "585f6066ae144797a64ed6a665d4a876": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bcc0eee9f114fa09641d4eb31f4fe50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eca86ac452bf404598af288b242bb830",
              "IPY_MODEL_bacfb6c6b19a4ef0a1aeb9954e43a1bf",
              "IPY_MODEL_16daad3c524a4bb1b27d7397d995a168"
            ],
            "layout": "IPY_MODEL_fdd871f3ad2d4bd3a37d574f203842f1"
          }
        },
        "62bbeca5a45a493fac2fc2e5ad646978": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62fde0f465e942c4ac349980514f1575": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "639c305bf781491f8aa8ffaa2126348c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "644eab0db4fb4d70be915551d5e6a912": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6588358fdc3f4fb0844207f05ec2b66c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65ad0148a17f4ed4a260fe92c07aa79b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65d3229d5b7f4262920b2a81ac6f1041": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66a8cf97d45a4e68a67a6fa4bc60a1e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69da365bcb2c4760a2bdb3cbe752afbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6da37c59ba50438a97ade5438fec01c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72d7d98dd5d643bead124d02cad32fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74f482d67ab34391a0202b516ce50abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66a8cf97d45a4e68a67a6fa4bc60a1e0",
            "placeholder": "​",
            "style": "IPY_MODEL_7a1cb318cb5f4b3db8a6e53bc2757a87",
            "value": "Downloading: 100%"
          }
        },
        "75a33f7f513e465e913cf9ff20b90aa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d65e4f4a975746aab6abb40d4ce9e205",
            "placeholder": "​",
            "style": "IPY_MODEL_24ad04dff1a84f18b4dc97a1cb6c60d8",
            "value": " 29.0/29.0 [00:00&lt;00:00, 1.27kB/s]"
          }
        },
        "769f8745902c4e5687107f419d99ab53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74f482d67ab34391a0202b516ce50abf",
              "IPY_MODEL_40427afdae0143cfb154d1fd54e8775d",
              "IPY_MODEL_3a874ca4cc4f4d969964ae053d97af6d"
            ],
            "layout": "IPY_MODEL_c01b42adf48d4eccb1a12b6d45e8a991"
          }
        },
        "7a1cb318cb5f4b3db8a6e53bc2757a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a64308827884f6cb5c9a0c7dc8fcb9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b735684c5f54841bb52099fc52a62ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7df74a0a26714140b25b1f4f6b5f0a96": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e32c13de76f43e486a412a8740fddc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82f17606b4da47f494f40c9b61dd6691": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "867517bb807d4d18bef22a2ea8b0e4b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88ddc7a84b694f2fb6364be622ef54a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8927fdd488ce4e12a0c77234f3a66b08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c353a482db14c0aad02599661668bdf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93955f9043c14723b4ba1626b1bbb9ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93f85af45ad54af9b85a7091711d5b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b38bc2635b3d440098b299be3a7a6c3a",
              "IPY_MODEL_da5069e2643b483c9eb4a395599747a1",
              "IPY_MODEL_ed3b02edff5642f6817fe27f25fc3c02"
            ],
            "layout": "IPY_MODEL_143d82f3b37c4af2b3daf6bc641a120e"
          }
        },
        "95d774cabf2144629e338670d76bf92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_055c0e3fa0884689b3d1d883b64cd54d",
              "IPY_MODEL_01270f380dc5465491a83e3e164618a1",
              "IPY_MODEL_75a33f7f513e465e913cf9ff20b90aa4"
            ],
            "layout": "IPY_MODEL_82f17606b4da47f494f40c9b61dd6691"
          }
        },
        "9841715ffff243c7a424f37b3e831dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99a2c3adb9554d9f892a16fb5cef0688": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_585f6066ae144797a64ed6a665d4a876",
            "placeholder": "​",
            "style": "IPY_MODEL_c151e5003ee446cab6b83ca260ff7f4f",
            "value": " 570/570 [00:00&lt;00:00, 32.2kB/s]"
          }
        },
        "a252a7131a2a407fa060b0257025aab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a62a74b5db764b1ab8b6f4d456386b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fead061b85314144a53f2d636ccb981b",
            "placeholder": "​",
            "style": "IPY_MODEL_018a17804b2d4c729b317551546d9ff1",
            "value": "Downloading: 100%"
          }
        },
        "a6df1233579d4f46a2ce378f595872d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac0dd98e18394c428682d29a9edc1d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af1043ab0a0a4db7a0b75b10aa26a2d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b38bc2635b3d440098b299be3a7a6c3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_005c43efd7744d6d88f5ed3afe6c5264",
            "placeholder": "​",
            "style": "IPY_MODEL_15ce3d4a9e2346b6943dc5a092f5c6b9",
            "value": "Downloading: 100%"
          }
        },
        "b871755546f347329a08e25b25bc846d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5aeb6635690468bb88685b815aad3d4",
            "max": 1520013706,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_377c24f38a534b95b3f5b030a58479e2",
            "value": 1520013706
          }
        },
        "bacfb6c6b19a4ef0a1aeb9954e43a1bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6588358fdc3f4fb0844207f05ec2b66c",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbf34fb2c2be4998894c2413777dcd97",
            "value": 456318
          }
        },
        "c01b42adf48d4eccb1a12b6d45e8a991": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c151e5003ee446cab6b83ca260ff7f4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3c0ccc423644429a2d5f5cd486d77f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbf34fb2c2be4998894c2413777dcd97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccf7480376624033ad9b5d9231b502c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfcb10243eab4af69ec851536e999288": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88ddc7a84b694f2fb6364be622ef54a4",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2cfb58a8ad9744648de87a556720306f",
            "value": 570
          }
        },
        "d018ce7778b74fbbbf19ca23b392f792": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d65e4f4a975746aab6abb40d4ce9e205": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9a3dad4c264437d9ba109cdfa7d7be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b735684c5f54841bb52099fc52a62ec",
            "placeholder": "​",
            "style": "IPY_MODEL_a252a7131a2a407fa060b0257025aab1",
            "value": " 1.42G/1.42G [01:38&lt;00:00, 17.0MB/s]"
          }
        },
        "da5069e2643b483c9eb4a395599747a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b88687aba0e4c69aaff74da9ddffe9a",
            "max": 718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7d22abb73e14876bbdde13e7374bb85",
            "value": 718
          }
        },
        "e2a8de42e45a474fb35a60405a3db5f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65d3229d5b7f4262920b2a81ac6f1041",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38964e8bc5434b00abed61ac76fc0635",
            "value": 1042301
          }
        },
        "e5aeb6635690468bb88685b815aad3d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e91e4d45e86341e3b2c8d5b8cc6fe9d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9b28a59b0a844379ab7c2b9cb1b7e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fdbf7d63b202456b966bfbf9fced4d84",
              "IPY_MODEL_e2a8de42e45a474fb35a60405a3db5f4",
              "IPY_MODEL_fc85ea168655423193b72a1a3b081185"
            ],
            "layout": "IPY_MODEL_62bbeca5a45a493fac2fc2e5ad646978"
          }
        },
        "ea12bb6b930244b9a86a1beebac3aee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6da37c59ba50438a97ade5438fec01c9",
            "placeholder": "​",
            "style": "IPY_MODEL_d018ce7778b74fbbbf19ca23b392f792",
            "value": " 416M/416M [00:09&lt;00:00, 65.6MB/s]"
          }
        },
        "eca86ac452bf404598af288b242bb830": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_867517bb807d4d18bef22a2ea8b0e4b7",
            "placeholder": "​",
            "style": "IPY_MODEL_c3c0ccc423644429a2d5f5cd486d77f5",
            "value": "Downloading: 100%"
          }
        },
        "ed3b02edff5642f6817fe27f25fc3c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_644eab0db4fb4d70be915551d5e6a912",
            "placeholder": "​",
            "style": "IPY_MODEL_449be78d7b094bdd8abc81290fb26d0c",
            "value": " 718/718 [00:00&lt;00:00, 48.8kB/s]"
          }
        },
        "f7d22abb73e14876bbdde13e7374bb85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc5401ee05314946bb02d23e6022ca60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc85ea168655423193b72a1a3b081185": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8927fdd488ce4e12a0c77234f3a66b08",
            "placeholder": "​",
            "style": "IPY_MODEL_93955f9043c14723b4ba1626b1bbb9ea",
            "value": " 0.99M/0.99M [00:00&lt;00:00, 1.91MB/s]"
          }
        },
        "fdb1029ea3ae4e8693ea43800e39964a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdbf7d63b202456b966bfbf9fced4d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7df74a0a26714140b25b1f4f6b5f0a96",
            "placeholder": "​",
            "style": "IPY_MODEL_fdb1029ea3ae4e8693ea43800e39964a",
            "value": "Downloading: 100%"
          }
        },
        "fdd871f3ad2d4bd3a37d574f203842f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fead061b85314144a53f2d636ccb981b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}